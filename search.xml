<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>这篇文章用来介绍怎样通过Github和hexo来搭建个人博客</title>
    <url>/2021/03/01/build-blog/</url>
    <content><![CDATA[<h3 id="前言：需要准备的工具"><a href="#前言：需要准备的工具" class="headerlink" title="前言：需要准备的工具"></a>前言：需要准备的工具</h3><ol>
<li>github账号</li>
<li>Git环境</li>
<li>Node.js环境（hexo基于Node.js）</li>
<li>Hexo博客框架</li>
<li>Next主题</li>
</ol>
<span id="more"></span>
<h3 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h3><h4 id="Github搭建个人仓库"><a href="#Github搭建个人仓库" class="headerlink" title="Github搭建个人仓库"></a>Github搭建个人仓库</h4><p>仓库名不能随便取名，应该命名为 ： 用户名.<a href="http://github.io/">http://github.io</a> 。这个用户名即你的Github的账号名，这是固定的写法，不容易出错。<br><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210302114609.png" alt="20210302114609"></p>
<p>建立仓库后暂时不要管。</p>
<h4 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h4><p>可以参考廖雪峰的git教程 <a href="https://www.liaoxuefeng.com/wiki/896043488029600">廖雪峰的Git教程</a></p>
<p>安装完成后在命令提示符中输入git –version验证是否安装成功</p>
<h4 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h4><ol>
<li><p>安装包下载</p>
<p> <a href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a> 这个网址下载速度快一点</p>
</li>
<li><p>检测是否安装成功</p>
<ol>
<li>检测Node.js是否安装成功，在命令行中输入 node -v</li>
<li>检测npm是否安装成功，在命令行中输入npm -v </li>
</ol>
</li>
</ol>
<h4 id="安装HExo"><a href="#安装HExo" class="headerlink" title="安装HExo"></a>安装HExo</h4><ol>
<li><p>在电脑里新建一个文件夹，用来存放建博客需要的文件</p>
</li>
<li><p>然后在这个文件夹里右键， 选择 Git Bash Here</p>
</li>
<li><p>使用npm命令安装hexo： npm install hexo-cli -g</p>
<blockquote>
<p>安装完后输入hexo -v验证是否安装成功</p>
</blockquote>
</li>
<li><p>对博客进行初始化： hexo init</p>
<blockquote>
<p>注：这里init后面可以跟文件夹名字，这样将会新建一个文件夹用来存放博客，但是我感觉没有必要新建一个</p>
</blockquote>
</li>
<li><p>接着输入npm install安装必备的组件</p>
</li>
<li><p>输入hexo g生成静态网页</p>
</li>
<li><p>然后输入hexo s打开本地服务器</p>
</li>
<li><p>然后浏览器打开<a href="http://localhost:4000/">localhost:4000/</a>, 查看网页</p>
</li>
<li><p>按ctrl+c关闭本地服务器</p>
</li>
</ol>
<h4 id="将Git与你的Github账号绑定"><a href="#将Git与你的Github账号绑定" class="headerlink" title="将Git与你的Github账号绑定"></a>将Git与你的Github账号绑定</h4><ol>
<li><p>鼠标右击打开Git Bash，设置user.name和user.email配置信息：</p>
<pre><code> &gt; git config --global user.name &quot;你的GitHub用户名&quot; 

 &gt; git config --global user.email &quot;你的GitHub注册邮箱&quot;
</code></pre>
</li>
<li><p>生成ssh密钥文件：</p>
<pre><code> &gt; ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;
</code></pre>
</li>
<li><p>然后直接三个回车即可，默认不需要设置密码。然后找到生成的.ssh的文件夹中的id_rsa.pub密钥，将内容全部复制</p>
</li>
</ol>
<ol start="4">
<li>打开GitHub_Settings_keys 页面，新建new SSH Key</li>
</ol>
<ol start="5">
<li><p>Title为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴进去，最后点击Add SSH key。在Git Bash中检测GitHub公钥设置是否成功，输入 ssh <a href="mailto:&#x67;&#105;&#116;&#64;&#103;&#105;&#116;&#104;&#x75;&#x62;&#x2e;&#99;&#x6f;&#109;">&#x67;&#105;&#116;&#64;&#103;&#105;&#116;&#104;&#x75;&#x62;&#x2e;&#99;&#x6f;&#109;</a> </p>
</li>
<li><p>打开博客根目录下的_config.yml文件，这是博客的配置文件，在这里你可以修改与博客相关的各种信息。</p>
<p> 修改最后一行的配置：</p>
<blockquote>
<p>deploy:<br>type: git<br>repository: 这里填GitHub里面的SSH链接<br>branch: master  </p>
</blockquote>
</li>
</ol>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210302125823.png" alt="20210302125823" width="80%" height=""/></div>

<h4 id="写文章，发布文章"><a href="#写文章，发布文章" class="headerlink" title="写文章，发布文章"></a>写文章，发布文章</h4><ol>
<li><p>首先在博客根目录下右键打开git bash，安装一个扩展npm i hexo-deployer-git</p>
</li>
<li><p>然后输入hexo new post “article title”，新建一篇文章  </p>
</li>
<li><p>然后打开D:\study\program\blog\source_posts的目录，可以发现下面多了一个文件夹和一个.md文件，一个用来存放你的图片等数据，另一个就是你的文章文件啦。</p>
</li>
<li><p>编写完markdown文件后，根目录下输入hexo g生成静态网页，然后输入hexo s可以本地预览效果，最后输入hexo d上传到github上。这时打开你的github.io主页就能看到发布的文章啦</p>
</li>
</ol>
<p>5.注：使用 hexo deploy 命令会同时调用 hexo generate 命令，在博客根目录生成一个 public 文件夹，里面的文件就是推送到 GitHub 上的文件。之后想要更新博客内容的话，建议首先使用 hexo clean 命令清除掉 public 文件夹，然后再使用 hexo deploy 推送</p>
<h4 id="修改Next主题"><a href="#修改Next主题" class="headerlink" title="修改Next主题"></a>修改Next主题</h4><ol>
<li>修改文章内链接文本样式</li>
</ol>
<p>修改Blog/themes/next/source/css/_common/components/post/post.styl，在末尾添加CSS样式:</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 文章内链接文本样式  </span></span><br><span class="line"><span class="selector-class">.post-body</span> <span class="selector-tag">p</span> <span class="selector-tag">a</span>&#123;  </span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#0593d3</span>; <span class="comment">//原始链接颜色  </span></span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line">  <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#0593d3</span>; <span class="comment">//底部分割线颜色</span></span><br><span class="line">  &amp;<span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#fc6423</span>; <span class="comment">//鼠标经过颜色</span></span><br><span class="line">    <span class="attribute">border-bottom</span>: none;</span><br><span class="line">    <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#fc6423</span>; <span class="comment">//底部分割线颜色</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>网页</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫环境安装</title>
    <url>/2021/03/01/spyder1/</url>
    <content><![CDATA[<h2 id="先介绍怎样安装分布式数据库MongoDB"><a href="#先介绍怎样安装分布式数据库MongoDB" class="headerlink" title="先介绍怎样安装分布式数据库MongoDB"></a>先介绍怎样安装分布式数据库MongoDB</h2><p>目的：便于存储非结构化的数据  </p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫 环境安装</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫的基本原理和基本用法</title>
    <url>/2021/03/02/spyder2/</url>
    <content><![CDATA[<h2 id="爬虫的基本用法"><a href="#爬虫的基本用法" class="headerlink" title="爬虫的基本用法"></a>爬虫的基本用法</h2><p>这篇文章介绍爬虫的基本原理和基本用法</p>
<span id="more"></span>
<h4 id="一、爬虫的基本流程"><a href="#一、爬虫的基本流程" class="headerlink" title="一、爬虫的基本流程"></a>一、爬虫的基本流程</h4><ol>
<li>发起请求<ul>
<li>通过HTTP库向目标网页发起request请求，等待服务器响应</li>
</ul>
</li>
<li>获取响应<ul>
<li>若服务器正常响应，会得到一个response，response的内容就是请求的网页的内容</li>
</ul>
</li>
<li>解析内容<ul>
<li>得到的html。可以用正则表达式，网页解析库进行解析</li>
</ul>
</li>
<li>保存数据<ul>
<li>可以存为文本，也可保存至数据库</li>
</ul>
</li>
</ol>
<h4 id="二、request请求包含什么？"><a href="#二、request请求包含什么？" class="headerlink" title="二、request请求包含什么？"></a>二、request请求包含什么？</h4><ol>
<li><p><strong>请求方式：</strong></p>
<ul>
<li>主要用GET，POST两种</li>
<li>另外还有HEAD、PUT、delete、options不常用</li>
</ul>
<p> <strong>注：get请求和post请求的区别</strong></p>
<ul>
<li>get请求：参数跟在url后面，可以通过url直接访问</li>
<li>post请求： 不能再直接通过url访问，需要提交类似于表单的请求，例如登陆验证，可以保证一定程度的信息安全。</li>
</ul>
</li>
<li><p><strong>请求url</strong></p>
<ul>
<li>url全称：统一资源定位符。如一个网页，一张图片都可以用url来唯一确定</li>
</ul>
</li>
<li><p><strong>请求头</strong></p>
<ul>
<li>包含请求时的头部信息：如User-Agent、 Host、 Cookies等信息</li>
</ul>
</li>
<li><p><strong>请求体</strong></p>
<ul>
<li>请求时额外携带的数据，如表单提交时的表单数据</li>
</ul>
</li>
</ol>
<h4 id="三、Response包含什么？"><a href="#三、Response包含什么？" class="headerlink" title="三、Response包含什么？"></a>三、Response包含什么？</h4><ol>
<li><p><strong>响应状态</strong><br>多种响应状态码：如200表示成功，301表示跳转，404找不到页面，502服务器错误</p>
</li>
<li><p><strong>响应头</strong></p>
<ul>
<li>如内容类型、内容长度、服务器信息、设置cookies维持会话等</li>
</ul>
</li>
<li><p><strong>响应体</strong><br>最主要的部分，包含了请求网页的源代码。</p>
</li>
</ol>
<h4 id="四、Requests库的get-方法"><a href="#四、Requests库的get-方法" class="headerlink" title="四、Requests库的get()方法"></a>四、Requests库的get()方法</h4><ol>
<li><p><strong>r = requests.get(url)</strong></p>
 <font color="blue">
 > 检测是否安装成功的方法  
 > import requests  
 > r=requests.get("http://www.baidu.com")  
 > print(r.status_code)
 > 要是返回的状态码是200，则说明访问成功
 </font>
</li>
<li><p><strong>Response对象的属性</strong></p>
<blockquote>
<p>r.status_code   —&gt; http请求的返回状态，200表示连接成功，404表示连接失败<br>r.text   —&gt;http响应内容的字符串形式，即网络页面的内容<br>r.encoding  —&gt;从HTTP header中获取响应内容的编码方式<br>r.appearent_encoding  —&gt; 从内容中分析出响应内容的编码方式  </p>
</blockquote>
</li>
</ol>
<h4 id="五、抓取的网页怎样解析？"><a href="#五、抓取的网页怎样解析？" class="headerlink" title="五、抓取的网页怎样解析？"></a>五、抓取的网页怎样解析？</h4><ol>
<li><p><strong>直接处理</strong><br>网页比较简单的情况可以直接处理</p>
</li>
<li><p><strong>json解析</strong></p>
</li>
<li><p><strong>正则表达式</strong><br>规则的字符串，用来提取出文本中，我们所需要的信息。</p>
</li>
<li><p><strong>BeautifulSoup解析库</strong></p>
</li>
<li><p><strong>PyQuery解析库</strong></p>
</li>
<li><p><strong>XPath解析库</strong></p>
</li>
</ol>
<h4 id="问题：为什么请求到的结果和网页显示的而不一样？"><a href="#问题：为什么请求到的结果和网页显示的而不一样？" class="headerlink" title="问题：为什么请求到的结果和网页显示的而不一样？"></a>问题：为什么请求到的结果和网页显示的而不一样？</h4><p>原始的html没有这些数据，很多数据是通过js加载出来的</p>
<p><strong>怎样解决javascript问题</strong></p>
<ol>
<li><p>分析Ajax请求<br>Ajax请求返回的是json格式的数据，翻遍提取</p>
</li>
<li><p>通过Selenium/WebDriver驱动浏览器模拟加载网页</p>
</li>
<li><p>通过Splash库<br>也是用来模拟加载网页的；在github搜索Splash</p>
</li>
<li><p>PyV8库，Ghost.py</p>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>urllib库基本使用</title>
    <url>/2021/03/07/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="urllib库基本使用"><a href="#urllib库基本使用" class="headerlink" title="urllib库基本使用"></a>urllib库基本使用</h2><p>urllib是python内置的http请求库，我们只需要关心请求所需要的头部信息，不需要关注底层的原理。</p>
<span id="more"></span>
<h3 id="一、什么是urilib"><a href="#一、什么是urilib" class="headerlink" title="一、什么是urilib"></a>一、什么是urilib</h3><p>urllib是python内置的HTTP请求库，包含以下几个模块：</p>
<ol>
<li><p><strong>urllib.request</strong><br> 请求模块：只需要给这个方法传入链接，便可以模拟实现访问网页</p>
</li>
<li><p><strong>urllib.error</strong><br>异常处理模块：如果请求错误，可以捕捉这些异常，进行重试或其它操作</p>
</li>
<li><p><strong>urllib.parse</strong><br>url解析模块：提供了很多url处理方法，比如拆分合并等方法</p>
</li>
<li><p><strong>urllib.robotparser</strong><br>主要用来识别网站的robot.txt文件，判断哪些网站是可以爬的，哪些是不可以爬的</p>
</li>
</ol>
<h3 id="二、网页的请求—urllib-request"><a href="#二、网页的请求—urllib-request" class="headerlink" title="二、网页的请求—urllib.request"></a>二、网页的请求—urllib.request</h3><ol>
<li><p><strong>基础的使用网页链接请求网页(get类型的请求)</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>, timeout=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>其中timeout用来设置超时（单位是秒），若在规定时间内请求没有得到响应的话，将会抛出异常,例如：   </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response=urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>, time=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">&#x27;TIME OUT&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p><strong>需要传入url和data的请求(post类型的请求)</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">data = <span class="built_in">bytes</span>(urllib.parse.urlencode(&#123;<span class="string">&#x27;word&#x27;</span>:<span class="string">&#x27;hello&#x27;</span>&#125;), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/post&#x27;</span>, data=data)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>想要发送一些更复杂的请求，例如想要加入headers</strong><br>这时<code>urllib.request.urlopen</code>已经无法满足我们的需求，没有办法传入这么多参数。我们接下来使用<code>urllib.request.Request</code></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line">url = <span class="string">&#x27;http://httpbin.org/post&#x27;</span>  <span class="comment">#请求url</span></span><br><span class="line">headers = &#123;  <span class="comment">#请求头</span></span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/4.0....&#x27;</span>       </span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;httpbin.org&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">dict</span>=&#123;  <span class="comment">#请求体             </span></span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;Germey&#x27;</span>    </span><br><span class="line">&#125;</span><br><span class="line">data = <span class="built_in">bytes</span>(parse.urlencode(<span class="built_in">dict</span>), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">&#x27;POST&#x27;</span>) <span class="comment">#请求方式</span></span><br><span class="line">response = request.urlopen(req)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="三、响应"><a href="#三、响应" class="headerlink" title="三、响应"></a>三、响应</h3><ol>
<li><strong>状态码，响应头</strong><br>响应里面包含两个重要的信息：<strong>状态码</strong>和<strong>响应头</strong>。这两个信息是用来判断响应是否成功的重要标志。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.python.org&#x27;</span>)</span><br><span class="line">print(response.status)  <span class="comment">#获取状态码 200表示响应成功</span></span><br><span class="line">print(response.getheasers())  <span class="comment">#获取响应头</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="四、高级用法，例如设置代理，处理cookies等操作"><a href="#四、高级用法，例如设置代理，处理cookies等操作" class="headerlink" title="四、高级用法，例如设置代理，处理cookies等操作"></a>四、高级用法，例如设置代理，处理cookies等操作</h3><p>需要用到urllib库中的Handler方法</p>
<ol>
<li><p><strong>设置代理</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">proxy_header = urllib.request.ProxyHandler(&#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>:<span class="string">&#x27;http://127.0.0.1:9743&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>:<span class="string">&#x27;https://127.0.0.1:9743&#x27;</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = urllib.request.build_opener(proxy_handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>设置cookies</strong><br>cookies是在客户端保存的，用来记录用户身份的文本文件。在做爬虫时，cookies主要用来维持登陆状态。</p>
<ol>
<li><strong>获取网页的cookies</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line">cookie = http.cookiejar.CookieJar()  <span class="comment">#将cookie声明为一个cookiejar的对象</span></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)  <span class="comment">#利用handler来处理对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)  <span class="comment">#调用open方法正常获取response之后，网页的cookie值就会被自动赋值给前面声明的cookie对象</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">&quot;=&quot;</span>+item.value)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><strong>将得到的cookie存入文本</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line">filename = <span class="string">&#x27;cookie.txt&#x27;</span></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)  </span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)  <span class="comment">#利用handler来处理对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>) </span><br><span class="line">cookies.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li>
<li><strong>从文本中读取cookies</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)  </span><br><span class="line">cookie.load(<span class="string">&#x27;cookie.txt&#x27;</span>, ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)  <span class="comment">#利用handler来处理对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>) </span><br></pre></td></tr></table></figure>
如果请求的网页时需要登陆才能看见的，那么有了cookie便可以看见登陆之后的页面</li>
</ol>
</li>
</ol>
<h3 id="五、异常处理—urllib-error"><a href="#五、异常处理—urllib-error" class="headerlink" title="五、异常处理—urllib.error"></a>五、异常处理—urllib.error</h3><ol>
<li><p><strong>URLError和HTTPError的区别</strong></p>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210307172227.png" alt="20210307172227" width="90%" height=""/></div>

<p><strong>URLError</strong>：仅有reason属性，捕捉error后可以打印出错信息<br><strong>HTTPError</strong>：是URLError的子类，有3个属性</p>
<ul>
<li>code</li>
<li>reason：</li>
<li>headers：打印响应的响应头信息</li>
</ul>
</li>
<li><p><strong>一般异常捕捉的写法</strong><br>这样写可以判断出是HTTPError还是URLError</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&#x27;request successfully&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>验证异常具体是什么原因</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="built_in">type</span>(e.reason))</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason, socket.timeout)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="六、URL解析—urlparse"><a href="#六、URL解析—urlparse" class="headerlink" title="六、URL解析—urlparse"></a>六、URL解析—urlparse</h3><ol>
<li><p><strong>urlparse</strong><br> 主要是进行url解析，将url分割为几个标准的部分。</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.urlparse <span class="keyword">import</span> urlparse</span><br><span class="line">result = urlparse(<span class="string">&#x27;http://www.baidu.com/index.html;user?id=5#comment&#x27;</span>, scheme=<span class="string">&#x27;https&#x27;</span>, allow_fragments=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p> 注：scheme是默认参数，只有当url中不含协议类型时，才会使用scheme的赋值作为协议类型</p>
</li>
<li><p><strong>urljoin</strong><br> 将两个url拼接起来</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line">print(urljoin(<span class="string">&#x27;http://www.baidu.com&#x27;</span>, <span class="string">&#x27;https://www.taobao.com/index.html&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p> 注：拼接时，若两个url有相同的字段，则以后面url的字段为准。例如前面url的协议字段为http，后面url协议字段为https，则拼接后为https。</p>
</li>
<li><p><strong>urlencode</strong><br> 可以将一个字典对象转换为get请求参数。</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germey&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;22&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">&#x27;http&quot;//www.baidu.com?&#x27;</span></span><br><span class="line">url = base_url+urlencode(params)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>软件的命令随手记</title>
    <url>/2021/03/02/%E8%BD%AF%E4%BB%B6%E5%91%BD%E4%BB%A4%E9%9A%8F%E6%89%8B%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="软件的命令随手记"><a href="#软件的命令随手记" class="headerlink" title="软件的命令随手记"></a>软件的命令随手记</h2><p>这个文档主要用来记各种软件使用过程重容易忘记的命令</p>
<span id="more"></span>

<h3 id="vscode相关"><a href="#vscode相关" class="headerlink" title="vscode相关"></a>vscode相关</h3><ol>
<li><p>在vscode中打开命令行： “ ctrl+shift+` ”  </p>
</li>
<li><p>编写markdown时，利用插件picgo插入图片快捷键，win10从剪贴板插入图片：<code>Ctrl+atl+u</code></p>
</li>
</ol>
<h3 id="hexo相关"><a href="#hexo相关" class="headerlink" title="hexo相关"></a>hexo相关</h3><ol>
<li><p>新建一个博客：<code>hexo new &quot;这里写个名字&quot;</code></p>
</li>
<li><p>清除public：<code>hexo clean</code> ; 生成网页：<code>hexo g</code> ; 本地预览：<code>hexo s</code> ; 部署：<code>hexo d</code>;</p>
</li>
</ol>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>软件命令</tag>
      </tags>
  </entry>
</search>
