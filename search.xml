<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BeautifulSoup库的基本用法</title>
    <url>/2021/03/09/BeautifulSoup%E5%BA%93%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<h2 id="BeautifulSoup库的基本用法"><a href="#BeautifulSoup库的基本用法" class="headerlink" title="BeautifulSoup库的基本用法"></a>BeautifulSoup库的基本用法</h2><p>BeautifulSoup是灵活又方便的网页解析库，处理高效，支持多种解析器。利用它不用编写正则表达式即可方便地实现网页信息地提取。</p>
<span id="more"></span>

<ol>
<li><strong>解析器的选择</strong><div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309074913.png" alt="20210309074913" width="90%" height=""/></div>
一般常用 `lxml` 解析器

</li>
</ol>
<h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="标签选择器"><a href="#标签选择器" class="headerlink" title="标签选择器"></a>标签选择器</h4><p>以下的代码演示，基于下面的html文档</p>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309080512.png" alt="20210309080512" width="80%" height=""/></div>

<ol>
<li><p><strong>选择元素</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">print(soup.title)   <span class="comment">#这里会把标签和标签里的内容一起输出</span></span><br><span class="line">print(soup.head)</span><br><span class="line">print(soup.p)</span><br></pre></td></tr></table></figure>
<p>注：这里有个问题需要注意，通过这种方式获取标签，如果文档中有多个这样的标签，返回的结果是第一个标签的内容，如上面我们通过<code>soup.p</code>获取<code>p</code>标签，而文档中有多个<code>p</code>标签，但是只返回了第一个<code>p</code>标签内容</p>
</li>
<li><p><strong>获取名称</strong><br>就是获取标签的名称，例如<code>title</code>, <code>head</code>, <code>p</code>等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">soup.tilte.name</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>获取属性</strong><br>属性是指：标签里面的属性，例如<code>&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;</code> 的<code>name</code>属性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.p.attrs[<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line">print(soup.p[<span class="string">&#x27;name&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p> 上面两种方式都可以获取p标签的name属性值</p>
</li>
<li><p><strong>获取内容</strong><br> 内容是指：标签中间的内容，例如<code>&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;</code>。内容是：<code>The Dormouse&#39;s story</code></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">soup.p.string</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>嵌套选择</strong><br>我们直接可以通过下面嵌套的方式获取<br><code>print(soup.head.title.string)</code><br>直接输出head里面的 <code>The Dormouse&#39;s story</code></p>
</li>
<li><p><strong>子节点和子孙节点</strong></p>
<ol>
<li>获取子节点—<code>contents</code> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">print(soup.p.contents)</span><br></pre></td></tr></table></figure>
 结果是将p标签下的所有子标签存入到了一个列表中</li>
<li>获取子节点—<code>children</code> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">print(soup.p.children)</span><br><span class="line"><span class="keyword">for</span> i,child <span class="keyword">in</span> <span class="built_in">enumerate</span>(soup.p.children):</span><br><span class="line">    print(i, child)</span><br></pre></td></tr></table></figure>
 通过<code>children</code>的方式也可以获取p标签下的所有子节点内容和通过contents获取的结果是一样的，但是不同的地方是<code>soup.p.children</code>是一个迭代对象，而不是列表，只能通过循环的方式获取素有的信息.</li>
<li>获取孙子节点<br> 通过<code>contents</code>以及<code>children</code>都是获取子节点，如果想要获取子孙节点可以通<code>descendants</code><br> <code>print(soup.descendants)</code>同时这种获取的结果也是一个迭代器</li>
</ol>
</li>
<li><p><strong>获取父节点</strong><br> 通过<code>soup.a.parent</code>就可以获取<code>a</code>节点的父节点<code>p</code>节点的信息</p>
</li>
<li><p><strong>获取兄弟节点</strong><br><code>soup.a.next_siblings</code> 获取后面所有的兄弟节点<br><code>soup.a.previous_siblings</code> 获取前面所有的兄弟节点<br><code>soup.a.next_sibling</code> 获取下一个兄弟标签<br><code>souo.a.previous_sinbling</code> 获取上一个兄弟标签</p>
</li>
</ol>
<p><strong>但是仅仅使用这种标签的选择方式是远远不能满足我们的需求的，下面介绍一些标准的选择器</strong></p>
<h4 id="标准选择器"><a href="#标准选择器" class="headerlink" title="标准选择器"></a>标准选择器</h4><p>以下的代码演示，基于下面的html文档</p>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309083653.png" alt="20210309083653" width="60%" height=""/></div>

<ol>
<li><p><strong>find_all(name,attrs,recursive,text,**kwargs)</strong><br>可以根据标签名，属性，内容查找文档</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.find_all(<span class="string">&#x27;ul&#x27;</span>):</span><br><span class="line">    print(ul.find_all(<span class="string">&#x27;li&#x27;</span>))   <span class="comment">#遍历所有的`ul`标签。并遍历`ul`标签下的所有`li`标签</span></span><br></pre></td></tr></table></figure>

<ol>
<li><strong>利用attrs通过属性名进行查找</strong><br>attrs可以传入字典的方式来查找标签，其键名是标签的属性名，键值是标签的属性值。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;list-1&#x27;</span>&#125;))  <span class="comment">#输出的是第一个 ul 标签</span></span><br></pre></td></tr></table></figure>
 有一些特殊的属性，可以不用attrs查找，直接进行查找，例如： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.find_all(<span class="built_in">id</span>=<span class="string">&#x27;list-1&#x27;</span>))</span><br><span class="line">print(soup.find_all(class_=<span class="string">&#x27;element&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p><strong>find(name,attrs,recursive,text,**kwargs)</strong><br> 和find_all方法用法完全一样，只不过find_all是返回所有的节点，find方法指返回第一个节点</p>
 <div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309085751.png" alt="20210309085751" width="90%" height=""/></div>

</li>
</ol>
<h4 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h4><p>BeautifulSoup提供了select方法，可以直接传入css选择器进行查找，相对于find_all方法更简单。</p>
<p>以下的代码演示，基于下面的html文档</p>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309090516.png" alt="20210309090516" width="50%" height=""/></div>

<ol>
<li><p>如果需要查找<code>class</code>，要在前面加<code>.</code>, 例如</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.select(<span class="string">&#x27;.panel .panel-heading&#x27;</span>))  <span class="comment">#输出了第二个&lt;div&gt;标签。 注意.panel后面有空格</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>直接选择标签</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.select(<span class="string">&#x27;ul li&#x27;</span>))  <span class="comment">#输出了所有ul标签里面的所有li标签</span></span><br></pre></td></tr></table></figure></li>
<li><p>用id选择，需要加<code>#</code>号</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.select(<span class="string">&#x27;#list-2 .element&#x27;</span>))  <span class="comment">#输出了list-2里面的所有element。注意list-2后面有空格</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>获取标签的属性</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ul = soup.select(<span class="string">&#x27;ul&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">print(ul[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line">print(ul.attrs(<span class="string">&#x27;id&#x27;</span>))  <span class="comment">#两种方法均可以输出第一个ul标签的id</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>获取标签的内容</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> soup.select(<span class="string">&#x27;li&#x27;</span>):</span><br><span class="line">    print(li.get_text())</span><br></pre></td></tr></table></figure>

</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>Request库的使用</title>
    <url>/2021/03/08/Request%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="Reuqest库的基本使用"><a href="#Reuqest库的基本使用" class="headerlink" title="Reuqest库的基本使用"></a>Reuqest库的基本使用</h2><p>urllib在使用时还是相对比较繁琐，比如处理代理和cookie等。<br>Requests是用python编写，基于urllib编写的，比urllib更加方便，可以节约我们大量的工作。</p>
<span id="more"></span>

<h3 id="一、requests各种请求方式"><a href="#一、requests各种请求方式" class="headerlink" title="一、requests各种请求方式"></a>一、requests各种请求方式</h3><p><strong>Response对象的属性</strong><br>    <code>response.status_code</code>   —&gt; http请求的返回状态，200表示连接成功，404表示连接失败<br>    <code>response.headers</code><br>    <code>response.cookies</code><br>    <code>response.url</code><br>    <code>response.text</code>   —&gt;http响应内容的字符串形式，即网络页面的内容<br>    <code>response.encoding</code>  —&gt;从HTTP header中获取响应内容的编码方式<br>    <code>response.appearent_encoding</code>  —&gt; 从内容中分析出响应内容的编码方式<br>    <code>response.content</code>  —&gt; 返回内容的二进制编码形式</p>
<h4 id="Get请求"><a href="#Get请求" class="headerlink" title="Get请求"></a>Get请求</h4><ol>
<li><p><strong>基本的GET请求</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  requests</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>带参数的GET请求</strong><br>可以用字典的形式传值，不用自己写url编码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;zhan&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>, params=data)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>获取二进制数据—例如图片，视频</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://github.com/favicon.ico&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;favicon.ico&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>添加headers</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozailla/5.0....&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://www.zhihu.com/explore&#x27;</span>, headers=headers)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h4><ol>
<li><strong>基本的post请求</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">data = &#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;gremey&#x27;</span>, <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;22&#x27;</span>&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozailla/5.0....&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(<span class="string">&#x27;http://httpbin.org/post&#x27;</span>, data=data, headers=headers)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="二、高级操作"><a href="#二、高级操作" class="headerlink" title="二、高级操作"></a>二、高级操作</h3><ol>
<li><p><strong>获取cookie</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> response = requests.get(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"> print(response.cookies)</span><br><span class="line"> <span class="keyword">for</span> key, value <span class="keyword">in</span> response.cookies.items():</span><br><span class="line">     print(key+<span class="string">&#x27;=&#x27;</span>+value)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>会话维持模拟登陆</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">s = requests.Session()    <span class="comment">#用session对象发起post请求，用来登陆</span></span><br><span class="line">s.get(<span class="string">&#x27;http://...&#x27;</span>)</span><br><span class="line">response = s.get(<span class="string">&#x27;http://&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>证书验证</strong><br>用requests请求一个网站时，如果网站是https协议，它会首先检测证书是否是合法的，如果检测证书是不合法，会直接抛出SSLError错误，使程序中断。</p>
<ul>
<li>直接取消证书验证，但还是有警告信息<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.12306.cn&#x27;</span>, verify=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>代理设置</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">proxies=&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://...&#x27;</span>, proxies=proxies)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>爬取网页的通用代码框架</strong><br> 加入了网络异常处理的部分，使代码更加完善<br> Requests库的异常有以下几个:  </p>
<pre><code> `requests.ConnectionError` ---&gt;网络连接异常，如DNS查询失败，拒绝连接等  
 `requests.HTTPError` ---&gt;HTTP错误异常  
 `requests.URLRequired` ---&gt;URL缺失异常  
 `requests.TooManyRedirects` ---&gt;超过最大重定向次数，产生定向异常  
 `requests.ConnectTimeout`  ---&gt;连接远程服务器超时异常  
 `requests.Timeout`  ---&gt;请求URL超时，产生超时异常 
</code></pre>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>Selenium库基本使用</title>
    <url>/2021/03/10/Selenium%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="Selenium库基本使用"><a href="#Selenium库基本使用" class="headerlink" title="Selenium库基本使用"></a>Selenium库基本使用</h2><h3 id="一、基本概念解释"><a href="#一、基本概念解释" class="headerlink" title="一、基本概念解释"></a>一、基本概念解释</h3><ol>
<li><strong>什么是AJAX？</strong><br>AJAX，全称Asynchronous JavaScript And XML，即异步的JavaScript和XML。通过在后台与服务器进行少量的数据交换，AJAX可以使网页实现异步更新，实现网页的动态渲染。</li>
</ol>
<span id="more"></span>

</bar>
    这意味着可以在不加载整个网页的情况下,对网页的某部分进行更新。而传统的网页如果需要更新内容，则必须重载整个网页页面。
</bar>
    使用AJAX加载的网页数据，虽然将数据渲染到了浏览器中，但在网页源代码中还是看不到通过ajax加载的数据，只能看到通过url加载的数据，这使得通过requests、urllib无法正常获取网页的全部数据。

<ol start="2">
<li><p><strong>获取ajax数据的两种方式</strong>:</p>
<ul>
<li>直接分析ajax调用的接口,然后通过代码请求这个接口</li>
</ul>
<p> <strong>优点</strong>:可以直接请求到数据，不需要做一些解析工作，代码量少，性能高。<br> <strong>缺点</strong>:分析接口比较复杂，特别是一些通过js混淆的接口，需要一定的js功底，容易被发现是爬虫。</p>
</bar>

<ul>
<li>使用selenium+driver（浏览器驱动）模拟浏览器行为获取数据</li>
</ul>
<p> <strong>优点</strong>:浏览器能请求到的数据，使用selenium也能请求到，爬虫 更稳定，且适用于所有类型的动态渲染网页。<br> <strong>缺点</strong>:代码量多，性能低。</p>
</li>
<li><p><strong>selenium+chromedriver获取动态数据</strong><br> selenium是一个自动化测试工具，可以模拟人类在浏览器上的一些行为，自动处理浏览器上的一些行为，比如点击、填充数据等。还可以获取浏览器当前呈现的页面源码，解决动态渲染网页的数据抓取，做到可见即可爬。</p>
</bar>
 而chromedriver是一个驱动Chrome浏览器的驱动程序，selenium使用它才能够驱动Chrome浏览器。
</bar>
 针对不同的浏览器有不同的驱动（driver）,比如Firefox的驱动geckodriver、IE的驱动IEdriver等，都可以配合selenium驱动对应的浏览器，本文以Chrome浏览器为例。



</li>
</ol>
<h3 id="二、基本用法讲解"><a href="#二、基本用法讲解" class="headerlink" title="二、基本用法讲解"></a>二、基本用法讲解</h3><h4 id="访问页面"><a href="#访问页面" class="headerlink" title="访问页面"></a>访问页面</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"><span class="comment">#请求网页（适用所有请求类型）</span></span><br><span class="line">browser.get(<span class="string">&#x27;https://www.baidu.com/&#x27;</span>)</span><br><span class="line"><span class="comment">#获取当前浏览器渲染后的HTML代码</span></span><br><span class="line">print( browser.page_source )  </span><br><span class="line"><span class="comment">#关闭当前页面</span></span><br><span class="line">driver.close()</span><br></pre></td></tr></table></figure>
<h4 id="查找节点"><a href="#查找节点" class="headerlink" title="查找节点"></a>查找节点</h4><p>Selenium 可以驱动浏览器完成各种操作，比如填充表单、模拟点击等。举个例子，当我们想要完成向某个输入框输入文字的操作时，首先需要知道这个输入框在哪，而 Selenium 提供了一系列查找节点的方法，我们可以用这些方法来获取想要的节点，以便执行下一步动作或者提取信息。</p>
<ol>
<li>单个节点<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">&#x27;https://www.taobao.com&#x27;</span>)</span><br><span class="line">input_first = browser.find_element_by_id(<span class="string">&#x27;q&#x27;</span>)</span><br><span class="line">input_second = browser.find_element_by_css_selector(<span class="string">&#x27;#q&#x27;</span>)</span><br><span class="line">input_third = browser.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;q&quot;]&#x27;</span>)</span><br><span class="line">print(input_first, input_second, input_third)</span><br><span class="line">browser.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>这里列出所有获取单个节点的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_element_by_id</span><br><span class="line">find_element_by_name</span><br><span class="line">find_element_by_xpath</span><br><span class="line">find_element_by_link_text</span><br><span class="line">find_element_by_partial_link_text</span><br><span class="line">find_element_by_tag_name</span><br><span class="line">find_element_by_class_name</span><br><span class="line">find_element_by_css_selector</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>查找多个节点</li>
</ol>
<h3 id="三、参考链接"><a href="#三、参考链接" class="headerlink" title="三、参考链接"></a>三、参考链接</h3><p><a href="https://www.jianshu.com/p/740004969675">Ajax数据爬取及selenium的使用详解</a><br><a href="https://selenium-python-zh.readthedocs.io/en/latest/">selenium官方文档</a></p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>这篇文章用来介绍怎样通过Github和hexo来搭建个人博客</title>
    <url>/2021/03/01/build-blog/</url>
    <content><![CDATA[<h3 id="前言：需要准备的工具"><a href="#前言：需要准备的工具" class="headerlink" title="前言：需要准备的工具"></a>前言：需要准备的工具</h3><ol>
<li>github账号</li>
<li>Git环境</li>
<li>Node.js环境（hexo基于Node.js）</li>
<li>Hexo博客框架</li>
<li>Next主题</li>
</ol>
<span id="more"></span>
<h3 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h3><h4 id="Github搭建个人仓库"><a href="#Github搭建个人仓库" class="headerlink" title="Github搭建个人仓库"></a>Github搭建个人仓库</h4><p>仓库名不能随便取名，应该命名为 ： 用户名.<a href="http://github.io/">http://github.io</a> 。这个用户名即你的Github的账号名，这是固定的写法，不容易出错。<br><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210302114609.png" alt="20210302114609"></p>
<p>建立仓库后暂时不要管。</p>
<h4 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h4><p>可以参考廖雪峰的git教程 <a href="https://www.liaoxuefeng.com/wiki/896043488029600">廖雪峰的Git教程</a></p>
<p>安装完成后在命令提示符中输入git –version验证是否安装成功</p>
<h4 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h4><ol>
<li><p>安装包下载</p>
<p> <a href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a> 这个网址下载速度快一点</p>
</li>
<li><p>检测是否安装成功</p>
<ol>
<li>检测Node.js是否安装成功，在命令行中输入 node -v</li>
<li>检测npm是否安装成功，在命令行中输入npm -v </li>
</ol>
</li>
</ol>
<h4 id="安装HExo"><a href="#安装HExo" class="headerlink" title="安装HExo"></a>安装HExo</h4><ol>
<li><p>在电脑里新建一个文件夹，用来存放建博客需要的文件</p>
</li>
<li><p>然后在这个文件夹里右键， 选择 Git Bash Here</p>
</li>
<li><p>使用npm命令安装hexo： npm install hexo-cli -g</p>
<blockquote>
<p>安装完后输入hexo -v验证是否安装成功</p>
</blockquote>
</li>
<li><p>对博客进行初始化： hexo init</p>
<blockquote>
<p>注：这里init后面可以跟文件夹名字，这样将会新建一个文件夹用来存放博客，但是我感觉没有必要新建一个</p>
</blockquote>
</li>
<li><p>接着输入npm install安装必备的组件</p>
</li>
<li><p>输入hexo g生成静态网页</p>
</li>
<li><p>然后输入hexo s打开本地服务器</p>
</li>
<li><p>然后浏览器打开<a href="http://localhost:4000/">localhost:4000/</a>, 查看网页</p>
</li>
<li><p>按ctrl+c关闭本地服务器</p>
</li>
</ol>
<h4 id="将Git与你的Github账号绑定"><a href="#将Git与你的Github账号绑定" class="headerlink" title="将Git与你的Github账号绑定"></a>将Git与你的Github账号绑定</h4><ol>
<li><p>鼠标右击打开Git Bash，设置user.name和user.email配置信息：</p>
<pre><code> &gt; git config --global user.name &quot;你的GitHub用户名&quot; 

 &gt; git config --global user.email &quot;你的GitHub注册邮箱&quot;
</code></pre>
</li>
<li><p>生成ssh密钥文件：</p>
<pre><code> &gt; ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;
</code></pre>
</li>
<li><p>然后直接三个回车即可，默认不需要设置密码。然后找到生成的.ssh的文件夹中的id_rsa.pub密钥，将内容全部复制</p>
</li>
</ol>
<ol start="4">
<li>打开GitHub_Settings_keys 页面，新建new SSH Key</li>
</ol>
<ol start="5">
<li><p>Title为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴进去，最后点击Add SSH key。在Git Bash中检测GitHub公钥设置是否成功，输入 ssh <a href="mailto:&#103;&#x69;&#x74;&#x40;&#x67;&#105;&#116;&#x68;&#x75;&#x62;&#46;&#99;&#111;&#x6d;">&#103;&#x69;&#x74;&#x40;&#x67;&#105;&#116;&#x68;&#x75;&#x62;&#46;&#99;&#111;&#x6d;</a> </p>
</li>
<li><p>打开博客根目录下的_config.yml文件，这是博客的配置文件，在这里你可以修改与博客相关的各种信息。</p>
<p> 修改最后一行的配置：</p>
<blockquote>
<p>deploy:<br>type: git<br>repository: 这里填GitHub里面的SSH链接<br>branch: master  </p>
</blockquote>
</li>
</ol>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210302125823.png" alt="20210302125823" width="80%" height=""/></div>

<h4 id="写文章，发布文章"><a href="#写文章，发布文章" class="headerlink" title="写文章，发布文章"></a>写文章，发布文章</h4><ol>
<li><p>首先在博客根目录下右键打开git bash，安装一个扩展npm i hexo-deployer-git</p>
</li>
<li><p>然后输入hexo new post “article title”，新建一篇文章  </p>
</li>
<li><p>然后打开D:\study\program\blog\source_posts的目录，可以发现下面多了一个文件夹和一个.md文件，一个用来存放你的图片等数据，另一个就是你的文章文件啦。</p>
</li>
<li><p>编写完markdown文件后，根目录下输入hexo g生成静态网页，然后输入hexo s可以本地预览效果，最后输入hexo d上传到github上。这时打开你的github.io主页就能看到发布的文章啦</p>
</li>
</ol>
<p>5.注：使用 hexo deploy 命令会同时调用 hexo generate 命令，在博客根目录生成一个 public 文件夹，里面的文件就是推送到 GitHub 上的文件。之后想要更新博客内容的话，建议首先使用 hexo clean 命令清除掉 public 文件夹，然后再使用 hexo deploy 推送</p>
<h4 id="修改Next主题"><a href="#修改Next主题" class="headerlink" title="修改Next主题"></a>修改Next主题</h4><ol>
<li>修改文章内链接文本样式</li>
</ol>
<p>修改Blog/themes/next/source/css/_common/components/post/post.styl，在末尾添加CSS样式:</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 文章内链接文本样式  </span></span><br><span class="line"><span class="selector-class">.post-body</span> <span class="selector-tag">p</span> <span class="selector-tag">a</span>&#123;  </span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#0593d3</span>; <span class="comment">//原始链接颜色  </span></span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line">  <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#0593d3</span>; <span class="comment">//底部分割线颜色</span></span><br><span class="line">  &amp;<span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#fc6423</span>; <span class="comment">//鼠标经过颜色</span></span><br><span class="line">    <span class="attribute">border-bottom</span>: none;</span><br><span class="line">    <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#fc6423</span>; <span class="comment">//底部分割线颜色</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>网页</tag>
      </tags>
  </entry>
  <entry>
    <title>mongodb的基本使用</title>
    <url>/2021/03/18/mongodb%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="mongodb的启动，连接，基本用法"><a href="#mongodb的启动，连接，基本用法" class="headerlink" title="mongodb的启动，连接，基本用法"></a>mongodb的启动，连接，基本用法</h2><span id="more"></span>

<h3 id="mongodb的安装"><a href="#mongodb的安装" class="headerlink" title="mongodb的安装"></a>mongodb的安装</h3><p><a href="https://cuiqingcai.com/5205.html">https://cuiqingcai.com/5205.html</a></p>
<h3 id="mongodb的启动"><a href="#mongodb的启动" class="headerlink" title="mongodb的启动"></a>mongodb的启动</h3><pre><code>开启服务：`net start MongoDb`

关闭服务：`net stop MongoDb`
</code></pre>
<h3 id="MongoDb的连接"><a href="#MongoDb的连接" class="headerlink" title="MongoDb的连接"></a>MongoDb的连接</h3><ol>
<li><p>创建mongodb连接对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">client=pymongo.MongoClient(host=<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">27017</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定数据库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">db = client.test   <span class="comment">#指定test数据库</span></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">db = client[<span class="string">&#x27;test&#x27;</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定集合<br>MongoDB 的每个数据库又包含许多集合（collection），它们类似于关系型数据库中的表.</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">collection = db.students   <span class="comment">#指定students集合</span></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">collection = db[<span class="string">&#x27;students&#x27;</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>插入数据</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">student = &#123;</span><br><span class="line">    <span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;20170101&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Jordan&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;male&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">result = collection.insert_one(student)   <span class="comment">#插入一条数据</span></span><br><span class="line">print(result)       <span class="comment">#返回InsertOneResult 对象</span></span><br><span class="line">print(result.inserted_id)   <span class="comment">#调用其 inserted_id 属性获取_id</span></span><br><span class="line"></span><br><span class="line">result = collection.insert([student1, student2])  <span class="comment">#插入多条数据</span></span><br></pre></td></tr></table></figure>
<p> 注：在 MongoDB 中，每条数据其实都有一个 _id 属性来唯一标识。如果没有显式指明该属性，MongoDB 会自动产生一个 ObjectId 类型的 _id 属性</p>
</li>
<li><p>查询<br> 插入数据后，我们可以利用 find_one 或 find 方法进行查询，其中 find_one 查询得到的是单个结果，find 则返回一个生成器对象。</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = collection.find_one(&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Mike&#x27;</span>&#125;) <span class="comment">#查询 name 为 Mike 的数据</span></span><br><span class="line"></span><br><span class="line">results = collection.find(&#123;<span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>&#125;)  <span class="comment">#使用 find 方法查询多条数据</span></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<p> 我们也可以根据 ObjectId 来查询，此时需要调用 bson 库里面的 objectid</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bson.objectid <span class="keyword">import</span> ObjectId</span><br><span class="line">result = collection.find_one(&#123;<span class="string">&#x27;_id&#x27;</span>: ObjectId(<span class="string">&#x27;593278c115c2602667ec6bae&#x27;</span>)&#125;)</span><br></pre></td></tr></table></figure></li>
<li><p>条件查询  </p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = collection.find(&#123;<span class="string">&#x27;age&#x27;</span>: &#123;<span class="string">&#x27;$gt&#x27;</span>: <span class="number">20</span>&#125;&#125;)   <span class="comment">#查询年龄大于 20 的数据</span></span><br></pre></td></tr></table></figure>
<p> 查询的条件键值已经不是单纯的数字了，而是一个字典，其键名为比较符号 $gt，意思是大于，键值为 20。</p>
<p> 常见的比较符号：</p>
 <div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210318155641.png" alt="20210318155641" width="80%" height=""/></div>
</li>
<li><p>计数</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">count = collection.find().count()  <span class="comment">#统计所有数据条数</span></span><br><span class="line">count = collection.find(&#123;<span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>&#125;).count()  <span class="comment">#统计符合某个条件的数据</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>排序</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = collection.find().sort(<span class="string">&#x27;name&#x27;</span>, pymongo.ASCENDING)  <span class="comment">#按名字升序</span></span><br></pre></td></tr></table></figure>
<p> 其中<code>pymongo.ASCENDING</code> 指定升序，<code>pymongo.DESCENDING</code> 降序</p>
</li>
<li><p>偏移（跳过） </p>
<p> 在某些情况下，我们可能只需要取某几个元素，这时可以利用 skip 方法偏移几个位置，比如偏移 2，就代表忽略前两个元素，得到第 3 个及以后的元素：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = collection.find().sort(<span class="string">&#x27;name&#x27;</span>, pymongo.ASCENDING).skip(<span class="number">2</span>)</span><br><span class="line">print([result[<span class="string">&#x27;name&#x27;</span>] <span class="keyword">for</span> result <span class="keyword">in</span> results])</span><br></pre></td></tr></table></figure>
<p> 还可以用 limit 方法指定要取的结果个数:</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = collection.find().sort(<span class="string">&#x27;name&#x27;</span>, pymongo.ASCENDING).skip(<span class="number">2</span>).limit(<span class="number">2</span>)  </span><br><span class="line"><span class="comment">#跳过前两个，限制获取两个结果，也就是获取第3，4个</span></span><br></pre></td></tr></table></figure>
<p> 值得注意的是，在数据量非常庞大的时候，比如在查询千万、亿级别的数据库时，最好不要使用大的偏移量，因为这样很可能导致内存溢出。</p>
</li>
<li><p>更新  </p>
<p>可以使用 update 方法，指定更新的条件和更新后的数据即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">condition = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Kevin&#x27;</span>&#125;</span><br><span class="line">student = collection.find_one(condition)</span><br><span class="line">student[<span class="string">&#x27;age&#x27;</span>] = <span class="number">25</span></span><br><span class="line">result = collection.update(condition, student)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>update_one 方法和 update_many 方法，用法更加严格，它们的第 2 个参数需要使用 $ 类型操作符作为字典的键名.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">condition = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Kevin&#x27;</span>&#125;</span><br><span class="line">student = collection.find_one(condition)</span><br><span class="line">student[<span class="string">&#x27;age&#x27;</span>] = <span class="number">26</span></span><br><span class="line">result = collection.update_one(condition, &#123;<span class="string">&#x27;$set&#x27;</span>: student&#125;)</span><br><span class="line">print(result)</span><br><span class="line">print(result.matched_count, result.modified_count)</span><br></pre></td></tr></table></figure>
<p>上面调用了 update_one 方法，使得第 2 个参数不能再直接传入修改后的字典，而是需要使用 {‘$set’: student} 这样的形式。</p>
</li>
</ol>
<p>11.删除<br>    直接调用 remove 方法指定删除的条件即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = collection.remove(&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Kevin&#x27;</span>&#125;)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>pyquery的基本使用</title>
    <url>/2021/03/16/pyquery%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="pyquery的基础用法"><a href="#pyquery的基础用法" class="headerlink" title="pyquery的基础用法"></a>pyquery的基础用法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(url=<span class="string">&#x27;https://cuiqingcai.com&#x27;</span>)     <span class="comment">#传url</span></span><br><span class="line">print(doc(<span class="string">&#x27;title))</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>

<h4 id="基础的CSS选择器用法"><a href="#基础的CSS选择器用法" class="headerlink" title="基础的CSS选择器用法"></a>基础的CSS选择器用法</h4><p>示例html</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;container&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">&quot;list&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link2.html&quot;</span>&gt;</span>second item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0 active&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link3.html&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;bold&quot;</span>&gt;</span>third item<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1 active&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link4.html&quot;</span>&gt;</span>fourth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link5.html&quot;</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>使用css选择器选择所有的&lt;li&gt;标签</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQurey <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">print(doc(<span class="string">&#x27;#container .list li&#x27;</span>))  <span class="comment"># 输出所有 li 标签</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> doc(<span class="string">&#x27;#container .list li&#x27;</span>).items():</span><br><span class="line">    print(item.text())    <span class="comment">#输出所有 li 标签的内容</span></span><br></pre></td></tr></table></figure>
<h4 id="常用的查找方法"><a href="#常用的查找方法" class="headerlink" title="常用的查找方法"></a>常用的查找方法</h4><ol>
<li><p>查找子节点<br> 查找子节点需要用到<code>find</code>方法，传入的参数是css选择器，例如：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">items = doc(<span class="string">&#x27;.list&#x27;</span>)</span><br><span class="line">lis = items.find(<span class="string">&#x27;li&#x27;</span>)   </span><br></pre></td></tr></table></figure>
<p> <code>find</code> 方法回将复合条件的子节点都返回，查找范围是节点的所有子孙节点。<br> 如果只想查找子节点，可以用<code>children</code>方法</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lis = items.children()</span><br><span class="line">lis = items.children(<span class="string">&#x27;.active&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>查找父节点</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lis = items.parent(<span class="string">&#x27;.items&#x27;</span>)   <span class="comment">#查找父节点</span></span><br><span class="line">lis = items.parents(<span class="string">&#x27;.items&#x27;</span>)   <span class="comment">#查找父节点，及祖先节点</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查找兄弟节点 <code>siblings()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc=pq(html)</span><br><span class="line">li=doc(<span class="string">&#x27;.list .item-0.active&#x27;</span>)</span><br><span class="line">print(li.siblings())</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="获取信息"><a href="#获取信息" class="headerlink" title="获取信息"></a>获取信息</h4><p>示例html:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;wrap&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;container&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">&quot;list&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link2.html&quot;</span>&gt;</span>second item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0 active&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link3.html&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;bold&quot;</span>&gt;</span>third item<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1 active&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link4.html&quot;</span>&gt;</span>fourth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link5.html&quot;</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>获取属性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">a = doc(<span class="string">&#x27;.item-0.active a&#x27;</span>)  <span class="comment">#选择class 为 item-0 和 active 的 li 节点内的 a 节点</span></span><br><span class="line">print(a, <span class="built_in">type</span>(a))</span><br><span class="line">print(a.attr(<span class="string">&#x27;href&#x27;</span>)) <span class="comment">#输出a节点的属性值</span></span><br></pre></td></tr></table></figure>
<p> 也可以通过调用 attr 属性来获取属性值，<code>print(a.attr.href)</code></p>
<p> 注意：当返回结果包含多个节点时，调用 attr 方法，只会得到第 1 个节点的属性。<br> 这种情况，如果想获取所有的 a 节点的属性，就要用到遍历了：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">a = doc(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> a.items():</span><br><span class="line">    print(item.attr(<span class="string">&#x27;href&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
<li><p>获取文本</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">a = doc(<span class="string">&#x27;.item-0.active a&#x27;</span>)</span><br><span class="line">print(a.text())   <span class="comment">#获取a节点的文本内容 third item</span></span><br></pre></td></tr></table></figure>
<p>获取节点内部的HTML：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">li = doc(<span class="string">&#x27;.item-0.active&#x27;</span>)</span><br><span class="line">print(li)</span><br><span class="line">print(li.html())  <span class="comment">#返回的是li节点内的所有html文本</span></span><br></pre></td></tr></table></figure>
<p>注意：如果你想要得到的结果是多个节点，并且需要获取每个节点的内部 HTML 文本，则需要遍历每个节点。而 text 方法不需要遍历就可以获取，它将所有节点取文本之后合并成一个字符串。</p>
</li>
<li><p>使用attr、text、html改变节点属性，内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">li = doc(<span class="string">&#x27;.item-0.active&#x27;</span>)</span><br><span class="line">print(li)</span><br><span class="line">li.attr(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;link&#x27;</span>)  <span class="comment">#修改属性，该方法的第 1 个参数为属性名，第 2 个参数为属性值</span></span><br><span class="line">print(li)</span><br><span class="line">li.text(<span class="string">&#x27;changed item&#x27;</span>)  <span class="comment">#调用 text 方法传入文本，li 节点内部的文本全被改为传入的字符串文本</span></span><br><span class="line">print(li)</span><br><span class="line">li.html(<span class="string">&#x27;&lt;span&gt;changed item&lt;/span&gt;&#x27;</span>) <span class="comment">#调用 html 方法传入 HTML 文本</span></span><br><span class="line">print(li)</span><br></pre></td></tr></table></figure>
<p> 所以说，使用 attr 方法时如果只传入第 1 个参数的属性名，则是获取这个属性值；如果传入第 2 个参数，可以用来修改属性值。使用 text 和 html 方法时如果不传参数，则是获取节点内纯文本和 HTML 文本，如果传入参数，则进行赋值。</p>
</li>
<li><p>remove方法 </p>
<p> remove 方法就是移除，它有时会为信息的提取带来非常大的便利<br>示例html：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;wrap&quot;</span>&gt;</span></span><br><span class="line">    Hello, World</span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>This is a paragraph.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>这时如果直接输出&lt;div&gt;节点的文本内容，会同时输出&lt;p&gt;节点的文本内容。可以先使用<code>remove</code>方法将<p>节点移除。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">doc = pq(html)</span><br><span class="line">wrap = doc(<span class="string">&#x27;.wrap&#x27;</span>)</span><br><span class="line">print(wrap.text()) <span class="comment">#输出Hello, World This is a paragraph.</span></span><br><span class="line"></span><br><span class="line">wrap.find(<span class="string">&#x27;p&#x27;</span>).remove()</span><br><span class="line">print(wrap.text()) <span class="comment">#输出 Hello, World</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>伪选择器</p>
<p>例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等</p>
</li>
</ol>
<p>关于 CSS 选择器的更多用法，可以参考 <a href="https://www.w3school.com.cn/css/index.asp">https://www.w3school.com.cn/css/index.asp</a></p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>python多线程与多进程</title>
    <url>/2021/03/16/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>对于网络爬虫这种IO密集型的任务来说，使用多线程回大大提高程序的整体爬取效率<br>python中实现多线程的模块叫<code>threading</code></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>爬虫环境安装</title>
    <url>/2021/03/01/spyder1/</url>
    <content><![CDATA[<h2 id="先介绍怎样安装分布式数据库MongoDB"><a href="#先介绍怎样安装分布式数据库MongoDB" class="headerlink" title="先介绍怎样安装分布式数据库MongoDB"></a>先介绍怎样安装分布式数据库MongoDB</h2><p>目的：便于存储非结构化的数据  </p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫 环境安装</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫的基本原理和基本用法</title>
    <url>/2021/03/02/spyder2/</url>
    <content><![CDATA[<h2 id="爬虫的基本用法"><a href="#爬虫的基本用法" class="headerlink" title="爬虫的基本用法"></a>爬虫的基本用法</h2><p>这篇文章介绍爬虫的基本原理和基本用法</p>
<span id="more"></span>
<h4 id="一、爬虫的基本流程"><a href="#一、爬虫的基本流程" class="headerlink" title="一、爬虫的基本流程"></a>一、爬虫的基本流程</h4><ol>
<li>发起请求<ul>
<li>通过HTTP库向目标网页发起request请求，等待服务器响应</li>
</ul>
</li>
<li>获取响应<ul>
<li>若服务器正常响应，会得到一个response，response的内容就是请求的网页的内容</li>
</ul>
</li>
<li>解析内容<ul>
<li>得到的html。可以用正则表达式，网页解析库进行解析</li>
</ul>
</li>
<li>保存数据<ul>
<li>可以存为文本，也可保存至数据库</li>
</ul>
</li>
</ol>
<h4 id="二、request请求包含什么？"><a href="#二、request请求包含什么？" class="headerlink" title="二、request请求包含什么？"></a>二、request请求包含什么？</h4><ol>
<li><p><strong>请求方式：</strong></p>
<ul>
<li>主要用GET，POST两种</li>
<li>另外还有HEAD、PUT、delete、options不常用</li>
</ul>
<p> <strong>注：get请求和post请求的区别</strong></p>
<ul>
<li>get请求：参数跟在url后面，可以通过url直接访问</li>
<li>post请求： 不能再直接通过url访问，需要提交类似于表单的请求，例如登陆验证，可以保证一定程度的信息安全。</li>
</ul>
</li>
<li><p><strong>请求url</strong></p>
<ul>
<li>url全称：统一资源定位符。如一个网页，一张图片都可以用url来唯一确定</li>
</ul>
</li>
<li><p><strong>请求头</strong></p>
<ul>
<li>包含请求时的头部信息：如User-Agent、 Host、 Cookies等信息</li>
</ul>
</li>
<li><p><strong>请求体</strong></p>
<ul>
<li>请求时额外携带的数据，如表单提交时的表单数据</li>
</ul>
</li>
</ol>
<h4 id="三、Response包含什么？"><a href="#三、Response包含什么？" class="headerlink" title="三、Response包含什么？"></a>三、Response包含什么？</h4><ol>
<li><p><strong>响应状态</strong><br>多种响应状态码：如200表示成功，301表示跳转，404找不到页面，502服务器错误</p>
</li>
<li><p><strong>响应头</strong></p>
<ul>
<li>如内容类型、内容长度、服务器信息、设置cookies维持会话等</li>
</ul>
</li>
<li><p><strong>响应体</strong><br>最主要的部分，包含了请求网页的源代码。</p>
</li>
</ol>
<h4 id="四、Requests库的get-方法"><a href="#四、Requests库的get-方法" class="headerlink" title="四、Requests库的get()方法"></a>四、Requests库的get()方法</h4><ol>
<li><p><strong>r = requests.get(url)</strong></p>
 <font color="blue">
 > 检测是否安装成功的方法  
 > import requests  
 > r=requests.get("http://www.baidu.com")  
 > print(r.status_code)
 > 要是返回的状态码是200，则说明访问成功
 </font>
</li>
<li><p><strong>Response对象的属性</strong></p>
<blockquote>
<p>r.status_code   —&gt; http请求的返回状态，200表示连接成功，404表示连接失败<br>r.text   —&gt;http响应内容的字符串形式，即网络页面的内容<br>r.encoding  —&gt;从HTTP header中获取响应内容的编码方式<br>r.appearent_encoding  —&gt; 从内容中分析出响应内容的编码方式  </p>
</blockquote>
</li>
</ol>
<h4 id="五、抓取的网页怎样解析？"><a href="#五、抓取的网页怎样解析？" class="headerlink" title="五、抓取的网页怎样解析？"></a>五、抓取的网页怎样解析？</h4><ol>
<li><p><strong>直接处理</strong><br>网页比较简单的情况可以直接处理</p>
</li>
<li><p><strong>json解析</strong></p>
</li>
<li><p><strong>正则表达式</strong><br>规则的字符串，用来提取出文本中，我们所需要的信息。</p>
</li>
<li><p><strong>BeautifulSoup解析库</strong></p>
</li>
<li><p><strong>PyQuery解析库</strong></p>
</li>
<li><p><strong>XPath解析库</strong></p>
</li>
</ol>
<h4 id="问题：为什么请求到的结果和网页显示的而不一样？"><a href="#问题：为什么请求到的结果和网页显示的而不一样？" class="headerlink" title="问题：为什么请求到的结果和网页显示的而不一样？"></a>问题：为什么请求到的结果和网页显示的而不一样？</h4><p>原始的html没有这些数据，很多数据是通过js加载出来的</p>
<p><strong>怎样解决javascript问题</strong></p>
<ol>
<li><p>分析Ajax请求<br>Ajax请求返回的是json格式的数据，翻遍提取</p>
</li>
<li><p>通过Selenium/WebDriver驱动浏览器模拟加载网页</p>
</li>
<li><p>通过Splash库<br>也是用来模拟加载网页的；在github搜索Splash</p>
</li>
<li><p>PyV8库，Ghost.py</p>
</li>
</ol>
<h4 id="六、网络基础知识"><a href="#六、网络基础知识" class="headerlink" title="六、网络基础知识"></a>六、网络基础知识</h4><ol>
<li><p><strong>Http和Https</strong>  </p>
<p>HTTP 的全称是 Hyper Text Transfer Protocol，中文名叫作超文本传输协议，HTTP 协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。HTTP 由万维网协会（World Wide Web Consortium）和 Internet 工作小组 IETF（Internet Engineering Task Force）共同合作制定的规范，目前广泛使用的是 HTTP 1.1 版本。</p>
<p>HTTPS 的全称是 Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标的 HTTP 通道，简单讲是 HTTP 的安全版，即 HTTP 下加入 SSL 层，简称为 HTTPS。HTTPS 的安全基础是 SSL，因此通过它传输的内容都是经过 SSL 加密的。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>urllib库基本使用</title>
    <url>/2021/03/07/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="urllib库基本使用"><a href="#urllib库基本使用" class="headerlink" title="urllib库基本使用"></a>urllib库基本使用</h2><p>urllib是python内置的http请求库，我们只需要关心请求所需要的头部信息，不需要关注底层的原理。</p>
<span id="more"></span>
<h3 id="一、什么是urilib"><a href="#一、什么是urilib" class="headerlink" title="一、什么是urilib"></a>一、什么是urilib</h3><p>urllib是python内置的HTTP请求库，包含以下几个模块：</p>
<ol>
<li><p><strong>urllib.request</strong><br> 请求模块：只需要给这个方法传入链接，便可以模拟实现访问网页</p>
</li>
<li><p><strong>urllib.error</strong><br>异常处理模块：如果请求错误，可以捕捉这些异常，进行重试或其它操作</p>
</li>
<li><p><strong>urllib.parse</strong><br>url解析模块：提供了很多url处理方法，比如拆分合并等方法</p>
</li>
<li><p><strong>urllib.robotparser</strong><br>主要用来识别网站的robot.txt文件，判断哪些网站是可以爬的，哪些是不可以爬的</p>
</li>
</ol>
<h3 id="二、网页的请求—urllib-request"><a href="#二、网页的请求—urllib-request" class="headerlink" title="二、网页的请求—urllib.request"></a>二、网页的请求—urllib.request</h3><ol>
<li><p><strong>基础的使用网页链接请求网页(get类型的请求)</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>, timeout=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>其中timeout用来设置超时（单位是秒），若在规定时间内请求没有得到响应的话，将会抛出异常,例如：   </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response=urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>, time=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">&#x27;TIME OUT&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p><strong>需要传入url和data的请求(post类型的请求)</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">data = <span class="built_in">bytes</span>(urllib.parse.urlencode(&#123;<span class="string">&#x27;word&#x27;</span>:<span class="string">&#x27;hello&#x27;</span>&#125;), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/post&#x27;</span>, data=data)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>想要发送一些更复杂的请求，例如想要加入headers</strong><br>这时<code>urllib.request.urlopen</code>已经无法满足我们的需求，没有办法传入这么多参数。我们接下来使用<code>urllib.request.Request</code></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line">url = <span class="string">&#x27;http://httpbin.org/post&#x27;</span>  <span class="comment">#请求url</span></span><br><span class="line">headers = &#123;  <span class="comment">#请求头</span></span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/4.0....&#x27;</span>       </span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;httpbin.org&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">dict</span>=&#123;  <span class="comment">#请求体             </span></span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;Germey&#x27;</span>    </span><br><span class="line">&#125;</span><br><span class="line">data = <span class="built_in">bytes</span>(parse.urlencode(<span class="built_in">dict</span>), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">&#x27;POST&#x27;</span>) <span class="comment">#请求方式</span></span><br><span class="line">response = request.urlopen(req)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="三、响应"><a href="#三、响应" class="headerlink" title="三、响应"></a>三、响应</h3><ol>
<li><strong>状态码，响应头</strong><br>响应里面包含两个重要的信息：<strong>状态码</strong>和<strong>响应头</strong>。这两个信息是用来判断响应是否成功的重要标志。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.python.org&#x27;</span>)</span><br><span class="line">print(response.status)  <span class="comment">#获取状态码 200表示响应成功</span></span><br><span class="line">print(response.getheasers())  <span class="comment">#获取响应头</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="四、高级用法，例如设置代理，处理cookies等操作"><a href="#四、高级用法，例如设置代理，处理cookies等操作" class="headerlink" title="四、高级用法，例如设置代理，处理cookies等操作"></a>四、高级用法，例如设置代理，处理cookies等操作</h3><p>需要用到urllib库中的Handler方法</p>
<ol>
<li><p><strong>设置代理</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">proxy_header = urllib.request.ProxyHandler(&#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>:<span class="string">&#x27;http://127.0.0.1:9743&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>:<span class="string">&#x27;https://127.0.0.1:9743&#x27;</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = urllib.request.build_opener(proxy_handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>设置cookies</strong><br>cookies是在客户端保存的，用来记录用户身份的文本文件。在做爬虫时，cookies主要用来维持登陆状态。</p>
<ol>
<li><strong>获取网页的cookies</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line">cookie = http.cookiejar.CookieJar()  <span class="comment">#将cookie声明为一个cookiejar的对象</span></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)  <span class="comment">#利用handler来处理对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)  <span class="comment">#调用open方法正常获取response之后，网页的cookie值就会被自动赋值给前面声明的cookie对象</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">&quot;=&quot;</span>+item.value)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><strong>将得到的cookie存入文本</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line">filename = <span class="string">&#x27;cookie.txt&#x27;</span></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)  </span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)  <span class="comment">#利用handler来处理对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>) </span><br><span class="line">cookies.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li>
<li><strong>从文本中读取cookies</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)  </span><br><span class="line">cookie.load(<span class="string">&#x27;cookie.txt&#x27;</span>, ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)  <span class="comment">#利用handler来处理对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>) </span><br></pre></td></tr></table></figure>
如果请求的网页时需要登陆才能看见的，那么有了cookie便可以看见登陆之后的页面</li>
</ol>
</li>
</ol>
<h3 id="五、异常处理—urllib-error"><a href="#五、异常处理—urllib-error" class="headerlink" title="五、异常处理—urllib.error"></a>五、异常处理—urllib.error</h3><ol>
<li><p><strong>URLError和HTTPError的区别</strong></p>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210307172227.png" alt="20210307172227" width="90%" height=""/></div>

<p><strong>URLError</strong>：仅有reason属性，捕捉error后可以打印出错信息<br><strong>HTTPError</strong>：是URLError的子类，有3个属性</p>
<ul>
<li>code</li>
<li>reason：</li>
<li>headers：打印响应的响应头信息</li>
</ul>
</li>
<li><p><strong>一般异常捕捉的写法</strong><br>这样写可以判断出是HTTPError还是URLError</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&#x27;request successfully&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>验证异常具体是什么原因</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="built_in">type</span>(e.reason))</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason, socket.timeout)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="六、URL解析—urlparse"><a href="#六、URL解析—urlparse" class="headerlink" title="六、URL解析—urlparse"></a>六、URL解析—urlparse</h3><ol>
<li><p><strong>urlparse</strong><br> 主要是进行url解析，将url分割为几个标准的部分。</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.urlparse <span class="keyword">import</span> urlparse</span><br><span class="line">result = urlparse(<span class="string">&#x27;http://www.baidu.com/index.html;user?id=5#comment&#x27;</span>, scheme=<span class="string">&#x27;https&#x27;</span>, allow_fragments=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p> 注：scheme是默认参数，只有当url中不含协议类型时，才会使用scheme的赋值作为协议类型</p>
</li>
<li><p><strong>urljoin</strong><br> 将两个url拼接起来</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line">print(urljoin(<span class="string">&#x27;http://www.baidu.com&#x27;</span>, <span class="string">&#x27;https://www.taobao.com/index.html&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p> 注：拼接时，若两个url有相同的字段，则以后面url的字段为准。例如前面url的协议字段为http，后面url协议字段为https，则拼接后为https。</p>
</li>
<li><p><strong>urlencode</strong><br> 可以将一个字典对象转换为get请求参数。</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germey&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;22&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">&#x27;http&quot;//www.baidu.com?&#x27;</span></span><br><span class="line">url = base_url+urlencode(params)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>xpath的基本使用</title>
    <url>/2021/03/10/xpath/</url>
    <content><![CDATA[<h2 id="介绍xpath的基本使用"><a href="#介绍xpath的基本使用" class="headerlink" title="介绍xpath的基本使用"></a>介绍xpath的基本使用</h2><p>XPath 是一门在 XML 文档中查找信息的语言。XPath 用于在 XML 文档中通过元素和属性进行导航。</p>
<span id="more"></span>

<p>在 XPath 中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。XML 文档是被作为节点树来对待的</p>
<h3 id="xpath语法"><a href="#xpath语法" class="headerlink" title="xpath语法"></a>xpath语法</h3><table>
    <tr>
        <th>符号</th>
        <th>作用</th>
    </tr>
    <tr bgcolor=#FFEBCD>
        <th>节点名称</th>
        <th>选取此节点的所有子节点</th>
    </tr>
    <tr>
        <th>/</th>
        <th>从根节点选取</th>
    </tr>
    <tr bgcolor=#FFEBCD>
        <th>//</th>
        <th>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置</th>
    </tr>
        <tr>
        <th>.</th>
        <th>选取当前节点</th>
    </tr>
    <tr bgcolor=#FFEBCD>
        <th>..</th>
        <th>    选取当前节点的父节点</th>
    </tr>
        <tr>
        <th>@</th>
        <th>选取属性</th>
    </tr>
</table>

<h4 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h4><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;?&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">bookstore</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">book</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">&quot;eng&quot;</span>&gt;</span>Harry Potter<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">price</span>&gt;</span>29.99<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">book</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">&quot;eng&quot;</span>&gt;</span>Learning XML<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">price</span>&gt;</span>39.95<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;/<span class="name">bookstore</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><code>xml.xpath(“bookstore”)</code> 表示选取 bookstore 元素的所有子节点<br><code>xml.xpath(“/bookstore”)</code> 表示选取根元素 bookstore。<br><code>xml.xpath(“bookstore/book”)</code> 选取属于 bookstore 的子元素的所有 book 元素。<br><code>xml.xpath(“//book”)</code> 选取所有 book 子元素，而不管它们在文档中的位置。<br><code>xml.xpath(“bookstore//book”)</code> 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。<br><code>xml.xpath(“//@lang”)</code> 选取名为 lang 的所有属性。</p>
<h4 id="按条件查询结果"><a href="#按条件查询结果" class="headerlink" title="按条件查询结果"></a>按条件查询结果</h4><div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210310214132.png" alt="20210310214132" width="100%" height=""/></div>

<h4 id="选取未知节点"><a href="#选取未知节点" class="headerlink" title="选取未知节点"></a>选取未知节点</h4><table>
    <tr>
        <th>符号</th>
        <th>作用</th>
        <th>例子</th>
        <th>结果</th>
    </tr>
    <tr bgcolor=#FFEBCD>
        <th>*</th>
        <th>匹配任何元素节点</th>
        <th>/bookstore/*</th>
        <th>选取 bookstore 元素的所有子元素</th>
    </tr>
    <tr>
        <th>@*</th>
        <th>匹配任何属性节点</th>
        <th>//*</th>
        <th>选取文档中的所有元素</th>
    </tr>
    <tr bgcolor=#FFEBCD>
        <th>node()</th>
        <th>匹配任何类型的节点</th>
        <th>//title[@*]</th>
        <th>选取所有带有属性的 title 元素</th>
    </tr>
</table>

<h4 id="选取若干路径"><a href="#选取若干路径" class="headerlink" title="选取若干路径"></a>选取若干路径</h4><p>通过在路径表达式中使用“|”运算符，您可以选取若干个路径。</p>
<p><code>//book/title | //book/price</code> 选取 book 元素的所有 title 和 price 元素。<br><code>//title | //price</code> 选取文档中的所有 title 和 price 元素。<br><code>/bookstore/book/title | //price</code> 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。</p>
<h4 id="一些函数"><a href="#一些函数" class="headerlink" title="一些函数"></a>一些函数</h4><ol>
<li><p><strong>starts-with函数</strong><br>获取以xxx开头的元素<br>例子：xpath(‘//div[stars-with(@class,”test”)]’)</p>
</li>
<li><p><strong>contains函数</strong><br>获取包含xxx的元素<br>例子：xpath(‘//div[contains(@id,”test”)]’)</p>
</li>
<li><p><strong>and</strong><br>与的关系<br>例子：xpath(‘//div[contains(@id,”test”) and contains(@id,”title”)]’)</p>
</li>
<li><p><strong>text()函数</strong><br>例子1：xpath(‘//div[contains(text(),”test”)]’)<br>例子2：xpath(‘//div[@id=”“test]/text()’)</p>
</li>
</ol>
<h4 id="内容来自于"><a href="#内容来自于" class="headerlink" title="内容来自于"></a>内容来自于</h4><p><a href="https://blog.csdn.net/Ka_Ka314/article/details/80997222">XPath — 用法总结整理</a></p>
<p><a href="https://www.w3school.com.cn/xpath/xpath_syntax.asp">w3school Xpath</a></p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>推荐系统架构</title>
    <url>/2021/03/09/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<h2 id="推荐系统设计"><a href="#推荐系统设计" class="headerlink" title="推荐系统设计"></a>推荐系统设计</h2><h3 id="推荐系统要素"><a href="#推荐系统要素" class="headerlink" title="推荐系统要素"></a>推荐系统要素</h3><pre><code>1. 前端界面
2. 数据(Lambda架构)
3. 业务知识
4. 算法
</code></pre>
<span id="more"></span>

<h4 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h4><div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309111304.png" alt="20210309111304" width="60%" height=""/></div>

<h3 id="大数据Lambda框架"><a href="#大数据Lambda框架" class="headerlink" title="大数据Lambda框架"></a>大数据Lambda框架</h3><ul>
<li>Lambda系统框架提供了一个结合实时数据和Hadoop数据环境的混合平台，混合实时计算和离线计算。</li>
<li>分层架构<ul>
<li>批处理层（离线处理）<ul>
<li>数据不可变，高延迟</li>
<li>日志收集  Flume</li>
<li>分布式存储 Hadoop hdfs</li>
<li>分布式计算 Hadoop MapReduce &amp; spark</li>
<li>存储数据库<ul>
<li>nosql</li>
<li>redis/memcache</li>
<li>mysql</li>
</ul>
</li>
</ul>
</li>
<li>实时处理层<ul>
<li>流式处理，持续计算</li>
<li>实时数据收集 flume &amp; kafka</li>
<li>实时数据分析 spark streaming/storm/flink</li>
</ul>
</li>
<li>服务层<ul>
<li>支持随机读</li>
<li>需要在短时间内返回结果</li>
<li>读取批处理层和实时处理层结果并进行归并</li>
</ul>
</li>
</ul>
</li>
<li>Lambda框架图<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309143757.png" alt="20210309143757" width="80%" height=""/></div>

</li>
</ul>
<p>P5</p>
]]></content>
      <categories>
        <category>推荐</category>
      </categories>
  </entry>
  <entry>
    <title>梯度下降与随机梯度下降</title>
    <url>/2021/05/11/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    <content><![CDATA[<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>软件的命令随手记</title>
    <url>/2021/03/02/%E8%BD%AF%E4%BB%B6%E5%91%BD%E4%BB%A4%E9%9A%8F%E6%89%8B%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="软件的命令随手记"><a href="#软件的命令随手记" class="headerlink" title="软件的命令随手记"></a>软件的命令随手记</h2><p>这个文档主要用来记各种软件使用过程重容易忘记的命令</p>
<span id="more"></span>

<h3 id="vscode相关"><a href="#vscode相关" class="headerlink" title="vscode相关"></a>vscode相关</h3><ol>
<li><p>在vscode中打开命令行： “ ctrl+` ”  </p>
</li>
<li><p>编写markdown时，利用插件picgo插入图片快捷键，win10从剪贴板插入图片：<code>Ctrl+shift+u</code> </p>
</li>
</ol>
<h3 id="hexo相关"><a href="#hexo相关" class="headerlink" title="hexo相关"></a>hexo相关</h3><ol>
<li><p>新建一个博客：<code>hexo new &quot;这里写个名字&quot;</code></p>
</li>
<li><p>清除public：<code>hexo clean</code> ; 生成网页：<code>hexo g</code> ; 本地预览：<code>hexo s</code> ; 部署：<code>hexo d</code>;</p>
</li>
</ol>
<h3 id="Marp相关"><a href="#Marp相关" class="headerlink" title="Marp相关"></a>Marp相关</h3><ol>
<li>marp预览不起作用： ctrl+shift+v</li>
</ol>
<h3 id="markdown相关"><a href="#markdown相关" class="headerlink" title="markdown相关"></a>markdown相关</h3><ol>
<li>markdown插入图片时，怎样为图片加入题注：<ul>
<li>直接在图片下方加入: &lt;center&gt;题注&lt;/center&gt;</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>软件命令</tag>
      </tags>
  </entry>
  <entry>
    <title>维护动态代理池</title>
    <url>/2021/03/11/%E7%BB%B4%E6%8A%A4%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%B1%A0/</url>
    <content><![CDATA[<h2 id="用Flask-Redis维护代理池"><a href="#用Flask-Redis维护代理池" class="headerlink" title="用Flask+Redis维护代理池"></a>用Flask+Redis维护代理池</h2><p>Redis主要用来维护池的队列存储，Flask来实现代理池的接口。<br>网上有大量免费的代理，但是有的好用，有的不好用，我们需要自己筛选好用的代理</p>
<span id="more"></span>

<h3 id="代理池的要求"><a href="#代理池的要求" class="headerlink" title="代理池的要求"></a>代理池的要求</h3><ol>
<li>多站抓取，异步检测</li>
<li>定时筛选，持续更新</li>
<li>提供接口，易于获取<br>最好是提供web形式的接口，便于提取</li>
</ol>
<h3 id="代理池的架构"><a href="#代理池的架构" class="headerlink" title="代理池的架构"></a>代理池的架构</h3><div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210311105240.png" alt="20210311105240" width="80%" height=""/></div>

<h3 id="代理池实现"><a href="#代理池实现" class="headerlink" title="代理池实现"></a>代理池实现</h3><p>github拉取源码，按教程修改。。。</p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>爬虫代码实战---仿猫眼top100</title>
    <url>/2021/03/19/%E7%88%AC%E8%99%AB%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98-%E4%BB%BF%E7%8C%AB%E7%9C%BCtop100/</url>
    <content><![CDATA[<h2 id="主要用来记录爬虫的代码"><a href="#主要用来记录爬虫的代码" class="headerlink" title="主要用来记录爬虫的代码"></a>主要用来记录爬虫的代码</h2><p>爬取的链接<a href="https://static1.scrape.cuiqingcai.com/">https://static1.scrape.cuiqingcai.com/</a><br>这个网站是仿的猫眼Top100</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO, <span class="built_in">format</span>=<span class="string">&#x27;%(asctime)s = %(levelname)s: %(message)s&#x27;</span>)</span><br><span class="line">Base_url = <span class="string">&#x27;https://static1.scrape.cuiqingcai.com&#x27;</span>   <span class="comment">#基础url</span></span><br><span class="line">total_page = <span class="number">10</span>   <span class="comment">#爬取的总页数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_page</span>(<span class="params">url</span>):</span>    <span class="comment">#爬取页面，返回页面的html，电影的详情页</span></span><br><span class="line">    logging.info(<span class="string">&#x27;scraping %s...&#x27;</span>,url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url,verify=<span class="literal">False</span>)    <span class="comment">#网页请求，取消证书验证警告</span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text   <span class="comment">#返回网页</span></span><br><span class="line">        logging.error(<span class="string">&#x27;get invalid status code %s while scraping %s&#x27;</span>, response.status_code, url)</span><br><span class="line">    <span class="keyword">except</span> requests.RequestException:</span><br><span class="line">        logging.error(<span class="string">&#x27;error occurred while scraping %s&#x27;</span>, url, exc_info=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_index</span>(<span class="params">page</span>):</span>  <span class="comment">#获取电影列表的页面，一个页面包含10个电影</span></span><br><span class="line">    index_url = <span class="string">f&#x27;<span class="subst">&#123;Base_url&#125;</span>/page/<span class="subst">&#123;page&#125;</span>&#x27;</span>    <span class="comment">#拼接url  https://static1.scrape.cuiqingcai.com/page/1</span></span><br><span class="line">    <span class="keyword">return</span> scrape_page(index_url)   <span class="comment">#获取页面</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_index</span>(<span class="params">html</span>):</span>    <span class="comment">#解析网页,获取该页面中10个电影的详情链接</span></span><br><span class="line">    doc = pq(html)</span><br><span class="line">    links = doc(<span class="string">&#x27;.el-card .name&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> links.items():</span><br><span class="line">        href = link.attr(<span class="string">&#x27;href&#x27;</span>)   <span class="comment">#获取详情页链接</span></span><br><span class="line">        detail_url = urljoin(Base_url, href)   <span class="comment">#链接拼接</span></span><br><span class="line">        <span class="comment">#logging.info(&#x27;get detail url %s&#x27;, detail_url)   #输出详情页链接</span></span><br><span class="line">        <span class="keyword">yield</span> detail_url</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_detila</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="keyword">return</span> scrape_page(url)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span>(<span class="params">html</span>):</span></span><br><span class="line">    doc = pq(html)</span><br><span class="line">    cover = doc(<span class="string">&#x27;img.cover&#x27;</span>).attr(<span class="string">&#x27;src&#x27;</span>)   <span class="comment">#获取封面图片</span></span><br><span class="line">    name = doc(<span class="string">&#x27;a&gt;h2&#x27;</span>).text()   <span class="comment">#直接选取 a 节点的直接子节点 h2 节点，并调用 text 方法提取其文本内容即可得到名称。</span></span><br><span class="line">    categories = [item.text() <span class="keyword">for</span> item <span class="keyword">in</span> doc(<span class="string">&#x27;.categories button span&#x27;</span>).items()]</span><br><span class="line">    publish_at = doc(<span class="string">&#x27;.info:contains(上映) &#x27;</span>).text()</span><br><span class="line">    publish_at = re.search(<span class="string">&#x27;(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)&#x27;</span>, publish_at).group(<span class="number">1</span>) <span class="keyword">if</span> publish_at <span class="keyword">and</span> re.search(<span class="string">&#x27;\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;&#x27;</span>, publish_at) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    drama = doc(<span class="string">&#x27;.drama p&#x27;</span>).text()</span><br><span class="line">    score = doc(<span class="string">&#x27;p.score&#x27;</span>).text()</span><br><span class="line">    score = <span class="built_in">float</span>(score) <span class="keyword">if</span> score <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;cover&#x27;</span>: cover,</span><br><span class="line">        <span class="string">&#x27;name&#x27;</span>: name,</span><br><span class="line">        <span class="string">&#x27;categories&#x27;</span>: categories,</span><br><span class="line">        <span class="string">&#x27;published_at&#x27;</span>: publish_at,</span><br><span class="line">        <span class="string">&#x27;drama&#x27;</span>: drama,</span><br><span class="line">        <span class="string">&#x27;score&#x27;</span>: score</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span>(<span class="params">data</span>):</span></span><br><span class="line">    collection.update_one(&#123;<span class="string">&#x27;name&#x27;</span>: data.get(<span class="string">&#x27;name&#x27;</span>)&#125;, &#123;<span class="string">&#x27;$set&#x27;</span>: data&#125;, upsert=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, total_page+<span class="number">1</span>):</span><br><span class="line">        index_html = scrape_index(page)</span><br><span class="line">        detail_urls = parse_index(index_html)</span><br><span class="line">        <span class="keyword">for</span> detail_url <span class="keyword">in</span> detail_urls:</span><br><span class="line">            detail_html = scrape_detila(detail_url)</span><br><span class="line">            data = parse_detail(detail_html)</span><br><span class="line">            logging.info(<span class="string">&#x27;detail data is %s&#x27;</span>, data)</span><br><span class="line">            save_data(data)</span><br><span class="line">            print(<span class="string">&quot;存储成功&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    MONGO_CONNECTION_STRING = <span class="string">&#x27;mongodb://localhost:27017&#x27;</span></span><br><span class="line">    MONGO_DB_NAME = <span class="string">&#x27;movies&#x27;</span></span><br><span class="line">    MONGO_COLLECTION_NAME = <span class="string">&#x27;movies&#x27;</span></span><br><span class="line">    client = pymongo.MongoClient(MONGO_CONNECTION_STRING)</span><br><span class="line">    db = client[<span class="string">&#x27;movies&#x27;</span>]</span><br><span class="line">    collection = db[<span class="string">&#x27;movies&#x27;</span>]</span><br><span class="line">    main()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫 代码</category>
      </categories>
      <tags>
        <tag>代码</tag>
      </tags>
  </entry>
</search>
