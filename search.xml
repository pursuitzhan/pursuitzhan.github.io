<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BeautifulSoup库的基本用法</title>
    <url>/2021/03/09/BeautifulSoup%E5%BA%93%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<h2 id="BeautifulSoup库的基本用法"><a href="#BeautifulSoup库的基本用法" class="headerlink" title="BeautifulSoup库的基本用法"></a>BeautifulSoup库的基本用法</h2><p>BeautifulSoup是灵活又方便的网页解析库，处理高效，支持多种解析器。利用它不用编写正则表达式即可方便地实现网页信息地提取。</p>
<span id="more"></span>

<ol>
<li><strong>解析器的选择</strong><div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309074913.png" alt="20210309074913" width="90%" height=""/></div>
一般常用 `lxml` 解析器

</li>
</ol>
<h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="标签选择器"><a href="#标签选择器" class="headerlink" title="标签选择器"></a>标签选择器</h4><p>以下的代码演示，基于下面的html文档</p>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309080512.png" alt="20210309080512" width="80%" height=""/></div>

<ol>
<li><p><strong>选择元素</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">print(soup.title)   <span class="comment">#这里会把标签和标签里的内容一起输出</span></span><br><span class="line">print(soup.head)</span><br><span class="line">print(soup.p)</span><br></pre></td></tr></table></figure>
<p>注：这里有个问题需要注意，通过这种方式获取标签，如果文档中有多个这样的标签，返回的结果是第一个标签的内容，如上面我们通过<code>soup.p</code>获取<code>p</code>标签，而文档中有多个<code>p</code>标签，但是只返回了第一个<code>p</code>标签内容</p>
</li>
<li><p><strong>获取名称</strong><br>就是获取标签的名称，例如<code>title</code>, <code>head</code>, <code>p</code>等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">soup.tilte.name</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>获取属性</strong><br>属性是指：标签里面的属性，例如<code>&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;</code> 的<code>name</code>属性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.p.attrs[<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line">print(soup.p[<span class="string">&#x27;name&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p> 上面两种方式都可以获取p标签的name属性值</p>
</li>
<li><p><strong>获取内容</strong><br> 内容是指：标签中间的内容，例如<code>&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;</code>。内容是：<code>The Dormouse&#39;s story</code></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">soup.p.string</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>嵌套选择</strong><br>我们直接可以通过下面嵌套的方式获取<br><code>print(soup.head.title.string)</code><br>直接输出head里面的 <code>The Dormouse&#39;s story</code></p>
</li>
<li><p><strong>子节点和子孙节点</strong></p>
<ol>
<li>获取子节点—<code>contents</code> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">print(soup.p.contents)</span><br></pre></td></tr></table></figure>
 结果是将p标签下的所有子标签存入到了一个列表中</li>
<li>获取子节点—<code>children</code> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">print(soup.p.children)</span><br><span class="line"><span class="keyword">for</span> i,child <span class="keyword">in</span> <span class="built_in">enumerate</span>(soup.p.children):</span><br><span class="line">    print(i, child)</span><br></pre></td></tr></table></figure>
 通过<code>children</code>的方式也可以获取p标签下的所有子节点内容和通过contents获取的结果是一样的，但是不同的地方是<code>soup.p.children</code>是一个迭代对象，而不是列表，只能通过循环的方式获取素有的信息.</li>
<li>获取孙子节点<br> 通过<code>contents</code>以及<code>children</code>都是获取子节点，如果想要获取子孙节点可以通<code>descendants</code><br> <code>print(soup.descendants)</code>同时这种获取的结果也是一个迭代器</li>
</ol>
</li>
<li><p><strong>获取父节点</strong><br> 通过<code>soup.a.parent</code>就可以获取<code>a</code>节点的父节点<code>p</code>节点的信息</p>
</li>
<li><p><strong>获取兄弟节点</strong><br><code>soup.a.next_siblings</code> 获取后面所有的兄弟节点<br><code>soup.a.previous_siblings</code> 获取前面所有的兄弟节点<br><code>soup.a.next_sibling</code> 获取下一个兄弟标签<br><code>souo.a.previous_sinbling</code> 获取上一个兄弟标签</p>
</li>
</ol>
<p><strong>但是仅仅使用这种标签的选择方式是远远不能满足我们的需求的，下面介绍一些标准的选择器</strong></p>
<h4 id="标准选择器"><a href="#标准选择器" class="headerlink" title="标准选择器"></a>标准选择器</h4><p>以下的代码演示，基于下面的html文档</p>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309083653.png" alt="20210309083653" width="60%" height=""/></div>

<ol>
<li><p><strong>find_all(name,attrs,recursive,text,**kwargs)</strong><br>可以根据标签名，属性，内容查找文档</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.find_all(<span class="string">&#x27;ul&#x27;</span>):</span><br><span class="line">    print(ul.find_all(<span class="string">&#x27;li&#x27;</span>))   <span class="comment">#遍历所有的`ul`标签。并遍历`ul`标签下的所有`li`标签</span></span><br></pre></td></tr></table></figure>

<ol>
<li><strong>利用attrs通过属性名进行查找</strong><br>attrs可以传入字典的方式来查找标签，其键名是标签的属性名，键值是标签的属性值。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;list-1&#x27;</span>&#125;))  <span class="comment">#输出的是第一个 ul 标签</span></span><br></pre></td></tr></table></figure>
 有一些特殊的属性，可以不用attrs查找，直接进行查找，例如： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.find_all(<span class="built_in">id</span>=<span class="string">&#x27;list-1&#x27;</span>))</span><br><span class="line">print(soup.find_all(class_=<span class="string">&#x27;element&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p><strong>find(name,attrs,recursive,text,**kwargs)</strong><br> 和find_all方法用法完全一样，只不过find_all是返回所有的节点，find方法指返回第一个节点</p>
 <div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309085751.png" alt="20210309085751" width="90%" height=""/></div>

</li>
</ol>
<h4 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h4><p>BeautifulSoup提供了select方法，可以直接传入css选择器进行查找，相对于find_all方法更简单。</p>
<p>以下的代码演示，基于下面的html文档</p>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210309090516.png" alt="20210309090516" width="50%" height=""/></div>

<ol>
<li><p>如果需要查找<code>class</code>，要在前面加<code>.</code>, 例如</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.select(<span class="string">&#x27;.panel .panel-heading&#x27;</span>))  <span class="comment">#输出了第二个&lt;div&gt;标签。 注意.panel后面有空格</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>直接选择标签</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.select(<span class="string">&#x27;ul li&#x27;</span>))  <span class="comment">#输出了所有ul标签里面的所有li标签</span></span><br></pre></td></tr></table></figure></li>
<li><p>用id选择，需要加<code>#</code>号</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(soup.select(<span class="string">&#x27;#list-2 .element&#x27;</span>))  <span class="comment">#输出了list-2里面的所有element。注意list-2后面有空格</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>获取标签的属性</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ul = soup.select(<span class="string">&#x27;ul&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">print(ul[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line">print(ul.attrs(<span class="string">&#x27;id&#x27;</span>))  <span class="comment">#两种方法均可以输出第一个ul标签的id</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>获取标签的内容</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> soup.select(<span class="string">&#x27;li&#x27;</span>):</span><br><span class="line">    print(li.get_text())</span><br></pre></td></tr></table></figure>

</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Request库的使用</title>
    <url>/2021/03/08/Request%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="Reuqest库的基本使用"><a href="#Reuqest库的基本使用" class="headerlink" title="Reuqest库的基本使用"></a>Reuqest库的基本使用</h2><p>urllib在使用时还是相对比较繁琐，比如处理代理和cookie等。<br>Requests是用python编写，基于urllib编写的，比urllib更加方便，可以节约我们大量的工作。</p>
<span id="more"></span>

<h3 id="一、requests各种请求方式"><a href="#一、requests各种请求方式" class="headerlink" title="一、requests各种请求方式"></a>一、requests各种请求方式</h3><p><strong>Response对象的属性</strong><br>    <code>response.status_code</code>   —&gt; http请求的返回状态，200表示连接成功，404表示连接失败<br>    <code>response.headers</code><br>    <code>response.cookies</code><br>    <code>response.url</code><br>    <code>response.text</code>   —&gt;http响应内容的字符串形式，即网络页面的内容<br>    <code>response.encoding</code>  —&gt;从HTTP header中获取响应内容的编码方式<br>    <code>response.appearent_encoding</code>  —&gt; 从内容中分析出响应内容的编码方式<br>    <code>response.content</code>  —&gt; 返回内容的二进制编码形式</p>
<h4 id="Get请求"><a href="#Get请求" class="headerlink" title="Get请求"></a>Get请求</h4><ol>
<li><p><strong>基本的GET请求</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  requests</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>带参数的GET请求</strong><br>可以用字典的形式传值，不用自己写url编码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;zhan&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>, params=data)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>获取二进制数据—例如图片，视频</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://github.com/favicon.ico&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;favicon.ico&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>添加headers</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozailla/5.0....&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://www.zhihu.com/explore&#x27;</span>, headers=headers)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h4><ol>
<li><strong>基本的post请求</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">data = &#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;gremey&#x27;</span>, <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;22&#x27;</span>&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozailla/5.0....&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(<span class="string">&#x27;http://httpbin.org/post&#x27;</span>, data=data, headers=headers)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="二、高级操作"><a href="#二、高级操作" class="headerlink" title="二、高级操作"></a>二、高级操作</h3><ol>
<li><p><strong>获取cookie</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> response = requests.get(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"> print(response.cookies)</span><br><span class="line"> <span class="keyword">for</span> key, value <span class="keyword">in</span> response.cookies.items():</span><br><span class="line">     print(key+<span class="string">&#x27;=&#x27;</span>+value)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>会话维持模拟登陆</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">s = requests.Session()    <span class="comment">#用session对象发起post请求，用来登陆</span></span><br><span class="line">s.get(<span class="string">&#x27;http://...&#x27;</span>)</span><br><span class="line">response = s.get(<span class="string">&#x27;http://&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>证书验证</strong><br>用requests请求一个网站时，如果网站是https协议，它会首先检测证书是否是合法的，如果检测证书是不合法，会直接抛出SSLError错误，使程序中断。</p>
<ul>
<li>直接取消证书验证，但还是有警告信息<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.12306.cn&#x27;</span>, verify=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>代理设置</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">proxies=&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://...&#x27;</span>, proxies=proxies)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>爬取网页的通用代码框架</strong><br> 加入了网络异常处理的部分，使代码更加完善<br> Requests库的异常有以下几个:</p>
<pre><code> `requests.ConnectionError` ---&gt;网络连接异常，如DNS查询失败，拒绝连接等  
 `requests.HTTPError` ---&gt;HTTP错误异常  
 `requests.URLRequired` ---&gt;URL缺失异常  
 `requests.TooManyRedirects` ---&gt;超过最大重定向次数，产生定向异常  
 `requests.ConnectTimeout`  ---&gt;连接远程服务器超时异常  
 `requests.Timeout`  ---&gt;请求URL超时，产生超时异常 
</code></pre>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>这篇文章用来介绍怎样通过Github和hexo来搭建个人博客</title>
    <url>/2021/03/01/build-blog/</url>
    <content><![CDATA[<h3 id="前言：需要准备的工具"><a href="#前言：需要准备的工具" class="headerlink" title="前言：需要准备的工具"></a>前言：需要准备的工具</h3><ol>
<li>github账号</li>
<li>Git环境</li>
<li>Node.js环境（hexo基于Node.js）</li>
<li>Hexo博客框架</li>
<li>Next主题</li>
</ol>
<span id="more"></span>
<h3 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h3><h4 id="Github搭建个人仓库"><a href="#Github搭建个人仓库" class="headerlink" title="Github搭建个人仓库"></a>Github搭建个人仓库</h4><p>仓库名不能随便取名，应该命名为 ： 用户名.<a href="http://github.io/">http://github.io</a> 。这个用户名即你的Github的账号名，这是固定的写法，不容易出错。<br><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210302114609.png" alt="20210302114609"></p>
<p>建立仓库后暂时不要管。</p>
<h4 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h4><p>可以参考廖雪峰的git教程 <a href="https://www.liaoxuefeng.com/wiki/896043488029600">廖雪峰的Git教程</a></p>
<p>安装完成后在命令提示符中输入git –version验证是否安装成功</p>
<h4 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h4><ol>
<li><p>安装包下载</p>
<p> <a href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a> 这个网址下载速度快一点</p>
</li>
<li><p>检测是否安装成功</p>
<ol>
<li>检测Node.js是否安装成功，在命令行中输入 node -v</li>
<li>检测npm是否安装成功，在命令行中输入npm -v </li>
</ol>
</li>
</ol>
<h4 id="安装HExo"><a href="#安装HExo" class="headerlink" title="安装HExo"></a>安装HExo</h4><ol>
<li><p>在电脑里新建一个文件夹，用来存放建博客需要的文件</p>
</li>
<li><p>然后在这个文件夹里右键， 选择 Git Bash Here</p>
</li>
<li><p>使用npm命令安装hexo： npm install hexo-cli -g</p>
<blockquote>
<p>安装完后输入hexo -v验证是否安装成功</p>
</blockquote>
</li>
<li><p>对博客进行初始化： hexo init</p>
<blockquote>
<p>注：这里init后面可以跟文件夹名字，这样将会新建一个文件夹用来存放博客，但是我感觉没有必要新建一个</p>
</blockquote>
</li>
<li><p>接着输入npm install安装必备的组件</p>
</li>
<li><p>输入hexo g生成静态网页</p>
</li>
<li><p>然后输入hexo s打开本地服务器</p>
</li>
<li><p>然后浏览器打开<a href="http://localhost:4000/">localhost:4000/</a>, 查看网页</p>
</li>
<li><p>按ctrl+c关闭本地服务器</p>
</li>
</ol>
<h4 id="将Git与你的Github账号绑定"><a href="#将Git与你的Github账号绑定" class="headerlink" title="将Git与你的Github账号绑定"></a>将Git与你的Github账号绑定</h4><ol>
<li><p>鼠标右击打开Git Bash，设置user.name和user.email配置信息：</p>
<pre><code> &gt; git config --global user.name &quot;你的GitHub用户名&quot; 

 &gt; git config --global user.email &quot;你的GitHub注册邮箱&quot;
</code></pre>
</li>
<li><p>生成ssh密钥文件：</p>
<pre><code> &gt; ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;
</code></pre>
</li>
<li><p>然后直接三个回车即可，默认不需要设置密码。然后找到生成的.ssh的文件夹中的id_rsa.pub密钥，将内容全部复制</p>
</li>
</ol>
<ol start="4">
<li>打开GitHub_Settings_keys 页面，新建new SSH Key</li>
</ol>
<ol start="5">
<li><p>Title为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴进去，最后点击Add SSH key。在Git Bash中检测GitHub公钥设置是否成功，输入 ssh <a href="mailto:&#x67;&#105;&#116;&#64;&#103;&#x69;&#116;&#x68;&#117;&#x62;&#x2e;&#99;&#111;&#x6d;">&#x67;&#105;&#116;&#64;&#103;&#x69;&#116;&#x68;&#117;&#x62;&#x2e;&#99;&#111;&#x6d;</a> </p>
</li>
<li><p>打开博客根目录下的_config.yml文件，这是博客的配置文件，在这里你可以修改与博客相关的各种信息。</p>
<p> 修改最后一行的配置：</p>
<blockquote>
<p>deploy:<br>type: git<br>repository: 这里填GitHub里面的SSH链接<br>branch: master  </p>
</blockquote>
</li>
</ol>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210302125823.png" alt="20210302125823" width="80%" height=""/></div>

<h4 id="写文章，发布文章"><a href="#写文章，发布文章" class="headerlink" title="写文章，发布文章"></a>写文章，发布文章</h4><ol>
<li><p>首先在博客根目录下右键打开git bash，安装一个扩展npm i hexo-deployer-git</p>
</li>
<li><p>然后输入hexo new post “article title”，新建一篇文章  </p>
</li>
<li><p>然后打开D:\study\program\blog\source_posts的目录，可以发现下面多了一个文件夹和一个.md文件，一个用来存放你的图片等数据，另一个就是你的文章文件啦。</p>
</li>
<li><p>编写完markdown文件后，根目录下输入hexo g生成静态网页，然后输入hexo s可以本地预览效果，最后输入hexo d上传到github上。这时打开你的github.io主页就能看到发布的文章啦</p>
</li>
</ol>
<p>5.注：使用 hexo deploy 命令会同时调用 hexo generate 命令，在博客根目录生成一个 public 文件夹，里面的文件就是推送到 GitHub 上的文件。之后想要更新博客内容的话，建议首先使用 hexo clean 命令清除掉 public 文件夹，然后再使用 hexo deploy 推送</p>
<h4 id="修改Next主题"><a href="#修改Next主题" class="headerlink" title="修改Next主题"></a>修改Next主题</h4><ol>
<li>修改文章内链接文本样式</li>
</ol>
<p>修改Blog/themes/next/source/css/_common/components/post/post.styl，在末尾添加CSS样式:</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 文章内链接文本样式  </span></span><br><span class="line"><span class="selector-class">.post-body</span> <span class="selector-tag">p</span> <span class="selector-tag">a</span>&#123;  </span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#0593d3</span>; <span class="comment">//原始链接颜色  </span></span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line">  <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#0593d3</span>; <span class="comment">//底部分割线颜色</span></span><br><span class="line">  &amp;<span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#fc6423</span>; <span class="comment">//鼠标经过颜色</span></span><br><span class="line">    <span class="attribute">border-bottom</span>: none;</span><br><span class="line">    <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#fc6423</span>; <span class="comment">//底部分割线颜色</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>网页</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫环境安装</title>
    <url>/2021/03/01/spyder1/</url>
    <content><![CDATA[<h2 id="先介绍怎样安装分布式数据库MongoDB"><a href="#先介绍怎样安装分布式数据库MongoDB" class="headerlink" title="先介绍怎样安装分布式数据库MongoDB"></a>先介绍怎样安装分布式数据库MongoDB</h2><p>目的：便于存储非结构化的数据  </p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫 环境安装</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫的基本原理和基本用法</title>
    <url>/2021/03/02/spyder2/</url>
    <content><![CDATA[<h2 id="爬虫的基本用法"><a href="#爬虫的基本用法" class="headerlink" title="爬虫的基本用法"></a>爬虫的基本用法</h2><p>这篇文章介绍爬虫的基本原理和基本用法</p>
<span id="more"></span>
<h4 id="一、爬虫的基本流程"><a href="#一、爬虫的基本流程" class="headerlink" title="一、爬虫的基本流程"></a>一、爬虫的基本流程</h4><ol>
<li>发起请求<ul>
<li>通过HTTP库向目标网页发起request请求，等待服务器响应</li>
</ul>
</li>
<li>获取响应<ul>
<li>若服务器正常响应，会得到一个response，response的内容就是请求的网页的内容</li>
</ul>
</li>
<li>解析内容<ul>
<li>得到的html。可以用正则表达式，网页解析库进行解析</li>
</ul>
</li>
<li>保存数据<ul>
<li>可以存为文本，也可保存至数据库</li>
</ul>
</li>
</ol>
<h4 id="二、request请求包含什么？"><a href="#二、request请求包含什么？" class="headerlink" title="二、request请求包含什么？"></a>二、request请求包含什么？</h4><ol>
<li><p><strong>请求方式：</strong></p>
<ul>
<li>主要用GET，POST两种</li>
<li>另外还有HEAD、PUT、delete、options不常用</li>
</ul>
<p> <strong>注：get请求和post请求的区别</strong></p>
<ul>
<li>get请求：参数跟在url后面，可以通过url直接访问</li>
<li>post请求： 不能再直接通过url访问，需要提交类似于表单的请求，例如登陆验证，可以保证一定程度的信息安全。</li>
</ul>
</li>
<li><p><strong>请求url</strong></p>
<ul>
<li>url全称：统一资源定位符。如一个网页，一张图片都可以用url来唯一确定</li>
</ul>
</li>
<li><p><strong>请求头</strong></p>
<ul>
<li>包含请求时的头部信息：如User-Agent、 Host、 Cookies等信息</li>
</ul>
</li>
<li><p><strong>请求体</strong></p>
<ul>
<li>请求时额外携带的数据，如表单提交时的表单数据</li>
</ul>
</li>
</ol>
<h4 id="三、Response包含什么？"><a href="#三、Response包含什么？" class="headerlink" title="三、Response包含什么？"></a>三、Response包含什么？</h4><ol>
<li><p><strong>响应状态</strong><br>多种响应状态码：如200表示成功，301表示跳转，404找不到页面，502服务器错误</p>
</li>
<li><p><strong>响应头</strong></p>
<ul>
<li>如内容类型、内容长度、服务器信息、设置cookies维持会话等</li>
</ul>
</li>
<li><p><strong>响应体</strong><br>最主要的部分，包含了请求网页的源代码。</p>
</li>
</ol>
<h4 id="四、Requests库的get-方法"><a href="#四、Requests库的get-方法" class="headerlink" title="四、Requests库的get()方法"></a>四、Requests库的get()方法</h4><ol>
<li><p><strong>r = requests.get(url)</strong></p>
 <font color="blue">
 > 检测是否安装成功的方法  
 > import requests  
 > r=requests.get("http://www.baidu.com")  
 > print(r.status_code)
 > 要是返回的状态码是200，则说明访问成功
 </font>
</li>
<li><p><strong>Response对象的属性</strong></p>
<blockquote>
<p>r.status_code   —&gt; http请求的返回状态，200表示连接成功，404表示连接失败<br>r.text   —&gt;http响应内容的字符串形式，即网络页面的内容<br>r.encoding  —&gt;从HTTP header中获取响应内容的编码方式<br>r.appearent_encoding  —&gt; 从内容中分析出响应内容的编码方式  </p>
</blockquote>
</li>
</ol>
<h4 id="五、抓取的网页怎样解析？"><a href="#五、抓取的网页怎样解析？" class="headerlink" title="五、抓取的网页怎样解析？"></a>五、抓取的网页怎样解析？</h4><ol>
<li><p><strong>直接处理</strong><br>网页比较简单的情况可以直接处理</p>
</li>
<li><p><strong>json解析</strong></p>
</li>
<li><p><strong>正则表达式</strong><br>规则的字符串，用来提取出文本中，我们所需要的信息。</p>
</li>
<li><p><strong>BeautifulSoup解析库</strong></p>
</li>
<li><p><strong>PyQuery解析库</strong></p>
</li>
<li><p><strong>XPath解析库</strong></p>
</li>
</ol>
<h4 id="问题：为什么请求到的结果和网页显示的而不一样？"><a href="#问题：为什么请求到的结果和网页显示的而不一样？" class="headerlink" title="问题：为什么请求到的结果和网页显示的而不一样？"></a>问题：为什么请求到的结果和网页显示的而不一样？</h4><p>原始的html没有这些数据，很多数据是通过js加载出来的</p>
<p><strong>怎样解决javascript问题</strong></p>
<ol>
<li><p>分析Ajax请求<br>Ajax请求返回的是json格式的数据，翻遍提取</p>
</li>
<li><p>通过Selenium/WebDriver驱动浏览器模拟加载网页</p>
</li>
<li><p>通过Splash库<br>也是用来模拟加载网页的；在github搜索Splash</p>
</li>
<li><p>PyV8库，Ghost.py</p>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>urllib库基本使用</title>
    <url>/2021/03/07/urllib%E5%BA%93%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="urllib库基本使用"><a href="#urllib库基本使用" class="headerlink" title="urllib库基本使用"></a>urllib库基本使用</h2><p>urllib是python内置的http请求库，我们只需要关心请求所需要的头部信息，不需要关注底层的原理。</p>
<span id="more"></span>
<h3 id="一、什么是urilib"><a href="#一、什么是urilib" class="headerlink" title="一、什么是urilib"></a>一、什么是urilib</h3><p>urllib是python内置的HTTP请求库，包含以下几个模块：</p>
<ol>
<li><p><strong>urllib.request</strong><br> 请求模块：只需要给这个方法传入链接，便可以模拟实现访问网页</p>
</li>
<li><p><strong>urllib.error</strong><br>异常处理模块：如果请求错误，可以捕捉这些异常，进行重试或其它操作</p>
</li>
<li><p><strong>urllib.parse</strong><br>url解析模块：提供了很多url处理方法，比如拆分合并等方法</p>
</li>
<li><p><strong>urllib.robotparser</strong><br>主要用来识别网站的robot.txt文件，判断哪些网站是可以爬的，哪些是不可以爬的</p>
</li>
</ol>
<h3 id="二、网页的请求—urllib-request"><a href="#二、网页的请求—urllib-request" class="headerlink" title="二、网页的请求—urllib.request"></a>二、网页的请求—urllib.request</h3><ol>
<li><p><strong>基础的使用网页链接请求网页(get类型的请求)</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>, timeout=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>其中timeout用来设置超时（单位是秒），若在规定时间内请求没有得到响应的话，将会抛出异常,例如：   </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response=urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>, time=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">&#x27;TIME OUT&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p><strong>需要传入url和data的请求(post类型的请求)</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">data = <span class="built_in">bytes</span>(urllib.parse.urlencode(&#123;<span class="string">&#x27;word&#x27;</span>:<span class="string">&#x27;hello&#x27;</span>&#125;), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/post&#x27;</span>, data=data)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>想要发送一些更复杂的请求，例如想要加入headers</strong><br>这时<code>urllib.request.urlopen</code>已经无法满足我们的需求，没有办法传入这么多参数。我们接下来使用<code>urllib.request.Request</code></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line">url = <span class="string">&#x27;http://httpbin.org/post&#x27;</span>  <span class="comment">#请求url</span></span><br><span class="line">headers = &#123;  <span class="comment">#请求头</span></span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/4.0....&#x27;</span>       </span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;httpbin.org&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">dict</span>=&#123;  <span class="comment">#请求体             </span></span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;Germey&#x27;</span>    </span><br><span class="line">&#125;</span><br><span class="line">data = <span class="built_in">bytes</span>(parse.urlencode(<span class="built_in">dict</span>), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">&#x27;POST&#x27;</span>) <span class="comment">#请求方式</span></span><br><span class="line">response = request.urlopen(req)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="三、响应"><a href="#三、响应" class="headerlink" title="三、响应"></a>三、响应</h3><ol>
<li><strong>状态码，响应头</strong><br>响应里面包含两个重要的信息：<strong>状态码</strong>和<strong>响应头</strong>。这两个信息是用来判断响应是否成功的重要标志。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.python.org&#x27;</span>)</span><br><span class="line">print(response.status)  <span class="comment">#获取状态码 200表示响应成功</span></span><br><span class="line">print(response.getheasers())  <span class="comment">#获取响应头</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="四、高级用法，例如设置代理，处理cookies等操作"><a href="#四、高级用法，例如设置代理，处理cookies等操作" class="headerlink" title="四、高级用法，例如设置代理，处理cookies等操作"></a>四、高级用法，例如设置代理，处理cookies等操作</h3><p>需要用到urllib库中的Handler方法</p>
<ol>
<li><p><strong>设置代理</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">proxy_header = urllib.request.ProxyHandler(&#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>:<span class="string">&#x27;http://127.0.0.1:9743&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>:<span class="string">&#x27;https://127.0.0.1:9743&#x27;</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = urllib.request.build_opener(proxy_handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>设置cookies</strong><br>cookies是在客户端保存的，用来记录用户身份的文本文件。在做爬虫时，cookies主要用来维持登陆状态。</p>
<ol>
<li><strong>获取网页的cookies</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line">cookie = http.cookiejar.CookieJar()  <span class="comment">#将cookie声明为一个cookiejar的对象</span></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)  <span class="comment">#利用handler来处理对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)  <span class="comment">#调用open方法正常获取response之后，网页的cookie值就会被自动赋值给前面声明的cookie对象</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">&quot;=&quot;</span>+item.value)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><strong>将得到的cookie存入文本</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line">filename = <span class="string">&#x27;cookie.txt&#x27;</span></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)  </span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)  <span class="comment">#利用handler来处理对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>) </span><br><span class="line">cookies.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li>
<li><strong>从文本中读取cookies</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)  </span><br><span class="line">cookie.load(<span class="string">&#x27;cookie.txt&#x27;</span>, ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)  <span class="comment">#利用handler来处理对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>) </span><br></pre></td></tr></table></figure>
如果请求的网页时需要登陆才能看见的，那么有了cookie便可以看见登陆之后的页面</li>
</ol>
</li>
</ol>
<h3 id="五、异常处理—urllib-error"><a href="#五、异常处理—urllib-error" class="headerlink" title="五、异常处理—urllib.error"></a>五、异常处理—urllib.error</h3><ol>
<li><p><strong>URLError和HTTPError的区别</strong></p>
<div align="center"><img src="https://raw.githubusercontent.com/pursuitzhan/picture/master/img/20210307172227.png" alt="20210307172227" width="90%" height=""/></div>

<p><strong>URLError</strong>：仅有reason属性，捕捉error后可以打印出错信息<br><strong>HTTPError</strong>：是URLError的子类，有3个属性</p>
<ul>
<li>code</li>
<li>reason：</li>
<li>headers：打印响应的响应头信息</li>
</ul>
</li>
<li><p><strong>一般异常捕捉的写法</strong><br>这样写可以判断出是HTTPError还是URLError</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&#x27;request successfully&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>验证异常具体是什么原因</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="built_in">type</span>(e.reason))</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason, socket.timeout)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="六、URL解析—urlparse"><a href="#六、URL解析—urlparse" class="headerlink" title="六、URL解析—urlparse"></a>六、URL解析—urlparse</h3><ol>
<li><p><strong>urlparse</strong><br> 主要是进行url解析，将url分割为几个标准的部分。</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.urlparse <span class="keyword">import</span> urlparse</span><br><span class="line">result = urlparse(<span class="string">&#x27;http://www.baidu.com/index.html;user?id=5#comment&#x27;</span>, scheme=<span class="string">&#x27;https&#x27;</span>, allow_fragments=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p> 注：scheme是默认参数，只有当url中不含协议类型时，才会使用scheme的赋值作为协议类型</p>
</li>
<li><p><strong>urljoin</strong><br> 将两个url拼接起来</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line">print(urljoin(<span class="string">&#x27;http://www.baidu.com&#x27;</span>, <span class="string">&#x27;https://www.taobao.com/index.html&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p> 注：拼接时，若两个url有相同的字段，则以后面url的字段为准。例如前面url的协议字段为http，后面url协议字段为https，则拼接后为https。</p>
</li>
<li><p><strong>urlencode</strong><br> 可以将一个字典对象转换为get请求参数。</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germey&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;22&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">&#x27;http&quot;//www.baidu.com?&#x27;</span></span><br><span class="line">url = base_url+urlencode(params)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>软件的命令随手记</title>
    <url>/2021/03/02/%E8%BD%AF%E4%BB%B6%E5%91%BD%E4%BB%A4%E9%9A%8F%E6%89%8B%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="软件的命令随手记"><a href="#软件的命令随手记" class="headerlink" title="软件的命令随手记"></a>软件的命令随手记</h2><p>这个文档主要用来记各种软件使用过程重容易忘记的命令</p>
<span id="more"></span>

<h3 id="vscode相关"><a href="#vscode相关" class="headerlink" title="vscode相关"></a>vscode相关</h3><ol>
<li><p>在vscode中打开命令行： “ ctrl+shift+` ”  </p>
</li>
<li><p>编写markdown时，利用插件picgo插入图片快捷键，win10从剪贴板插入图片：<code>Ctrl+atl+u</code></p>
</li>
</ol>
<h3 id="hexo相关"><a href="#hexo相关" class="headerlink" title="hexo相关"></a>hexo相关</h3><ol>
<li><p>新建一个博客：<code>hexo new &quot;这里写个名字&quot;</code></p>
</li>
<li><p>清除public：<code>hexo clean</code> ; 生成网页：<code>hexo g</code> ; 本地预览：<code>hexo s</code> ; 部署：<code>hexo d</code>;</p>
</li>
</ol>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>软件命令</tag>
      </tags>
  </entry>
</search>
